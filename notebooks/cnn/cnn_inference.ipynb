{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9044cc58",
   "metadata": {},
   "source": [
    "# CNN Inference and Test Set Metric Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877d6b9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d69a7d-5f0e-453a-8a7d-8ef4b100e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set working directory to root \n",
    "import os\n",
    "os.chdir(\"../../\")\n",
    "root = os.getcwd()\n",
    "root = root + \"/workspaces/mine-segmentation\" # for lightning studios\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34608fe0-9c89-4b39-b0b7-59d74efafdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import leafmap\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "from shapely.wkt import loads\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "from src.models.datamodule import MineDataModule\n",
    "from src.models.cnn.model import MineSegmentorCNN\n",
    "\n",
    "from src.visualization.visualization_funcs import plot_pred_vs_true_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873272f-89e7-48de-9115-7c9d21b62c1f",
   "metadata": {},
   "source": [
    "### Define paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea85c6-5086-42b2-b032-489890554d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2048 model\n",
    "# MINESEG_CHECKPOINT_PATH = (\"models/cnn/2048_mineseg-cnn_epoch-08_val-iou-0.5017.ckpt\")\n",
    "# CHIP_SIZE = 2048\n",
    "# TESTSET_BATCH_SIZE = 8\n",
    "# TRAIN_CHIP_DIR = \"data/processed/chips/npy/1024/train/chips/\"\n",
    "# TRAIN_LABEL_DIR = \" data/processed/chips/npy/1024/train/labels/\"\n",
    "# VAL_CHIP_DIR = \"data/processed/chips/npy/1024/val/chips/\"\n",
    "# VAL_LABEL_DIR = \"data/processed/chips/npy/1024/val/labels/\"\n",
    "# TEST_CHIP_DIR = \"data/processed/chips/npy/1024/test/chips/\"\n",
    "# TEST_LABEL_DIR = \"data/processed/chips/npy/1024/test/labels/\"\n",
    "\n",
    "# 1024 model\n",
    "MINESEG_CHECKPOINT_PATH = (\"models/cnn/1024_mineseg-cnn_epoch-15_val-iou-0.5290.ckpt\")\n",
    "CHIP_SIZE = 1024\n",
    "TESTSET_BATCH_SIZE = 16\n",
    "TRAIN_CHIP_DIR = \"data/processed/chips/npy/1024/train/chips/\"\n",
    "TRAIN_LABEL_DIR = \" data/processed/chips/npy/1024/train/labels/\"\n",
    "VAL_CHIP_DIR = \"data/processed/chips/npy/1024/val/chips/\"\n",
    "VAL_LABEL_DIR = \"data/processed/chips/npy/1024/val/labels/\"\n",
    "TEST_CHIP_DIR = \"data/processed/chips/npy/1024/test/chips/\"\n",
    "TEST_LABEL_DIR = \"data/processed/chips/npy/1024/test/labels/\"\n",
    "\n",
    "# # 512 model\n",
    "# MINESEG_CHECKPOINT_PATH = (\"models/cnn/512_mineseg-cnn_epoch-10_val-iou-0.5313.ckpt\")\n",
    "# CHIP_SIZE = 512\n",
    "# TESTSET_BATCH_SIZE = 64\n",
    "# TRAIN_CHIP_DIR = \"data/processed/chips/npy/512/train/chips/\"\n",
    "# TRAIN_LABEL_DIR = \"data/processed/chips/npy/512/train/labels/\"\n",
    "# VAL_CHIP_DIR = \"data/processed/chips/npy/512/val/chips/\"\n",
    "# VAL_LABEL_DIR = \"data/processed/chips/npy/512/val/labels/\"\n",
    "# TEST_CHIP_DIR = \"data/processed/chips/npy/512/test/chips/\"\n",
    "# TEST_LABEL_DIR = \"data/processed/chips/npy/512/test/labels/\"\n",
    "\n",
    "\n",
    "DATASET = \"/data/processed/mining_tiles_with_masks_and_bounding_boxes.gpkg\"\n",
    "METADATA_PATH = \"configs/cnn/cnn_segment_metadata.yaml\"\n",
    "BATCH_SIZE = 1\n",
    "if torch.cuda.is_available():\n",
    "    NUM_WORKERS = 16\n",
    "else:\n",
    "    NUM_WORKERS = 4\n",
    "PLATFORM = \"sentinel-2-l2a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_lightning = True\n",
    "if is_lightning:\n",
    "    MINESEG_CHECKPOINT_PATH = root + \"/\" + MINESEG_CHECKPOINT_PATH\n",
    "    METADATA_PATH = root + \"/\" + METADATA_PATH\n",
    "    TRAIN_CHIP_DIR = root +  \"/\" +TRAIN_CHIP_DIR\n",
    "    TRAIN_LABEL_DIR = root + \"/\" + TRAIN_LABEL_DIR\n",
    "    VAL_CHIP_DIR = root + \"/\" + VAL_CHIP_DIR\n",
    "    VAL_LABEL_DIR = root + \"/\" + VAL_LABEL_DIR\n",
    "    TEST_CHIP_DIR = root + \"/\" + TEST_CHIP_DIR\n",
    "    TEST_LABEL_DIR = root + \"/\" + TEST_LABEL_DIR\n",
    "    DATASET = root + \"/\" + DATASET\n",
    "    # print(f\"Using model at {MINESEG_CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MINESEG_CHECKPOINT_PATH.split(\"/\")[-1]\n",
    "print(f\"Using model {model_name}\")\n",
    "print(f\"Using chip size {CHIP_SIZE}\")\n",
    "print(f\"Using test chip dir {TEST_CHIP_DIR}\")\n",
    "print(f\"Using test label dir {TEST_LABEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc278db5-e241-4763-8f33-bdeb5b0f81fc",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0da577-f3e5-485a-bbc5-a3ff7367e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(checkpoint_path: str) -> MineSegmentorCNN:\n",
    "    # check if gpu is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    map_location=torch.device(device)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
    "    model_config = checkpoint[\"hyper_parameters\"]\n",
    "    model = MineSegmentorCNN.load_from_checkpoint(checkpoint_path, **model_config)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ba7fc-f1ca-465c-be66-15edca8e8419",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3402cf0a-cb9b-47c4-a12a-bb704912edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    train_chip_dir,\n",
    "    train_label_dir,\n",
    "    val_chip_dir,\n",
    "    val_label_dir,\n",
    "    test_chip_dir,\n",
    "    test_label_dir,\n",
    "    metadata_path,\n",
    "    batch_size,\n",
    "    num_workers,\n",
    "    platform,\n",
    "    data_augmentation,\n",
    "    index=None\n",
    "):\n",
    "    dm = MineDataModule(\n",
    "        train_chip_dir=train_chip_dir,\n",
    "        train_label_dir=train_label_dir,\n",
    "        val_chip_dir=val_chip_dir,\n",
    "        val_label_dir=val_label_dir,\n",
    "        test_chip_dir=test_chip_dir,\n",
    "        test_label_dir=test_label_dir,\n",
    "        metadata_path=metadata_path,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        platform=platform,\n",
    "        data_augmentation=data_augmentation,\n",
    "    )\n",
    "    \n",
    "    dm.setup(stage=\"test\")\n",
    "    \n",
    "    if index is not None:\n",
    "        test_dl = iter(dm.test_dataloader())\n",
    "        for i in range(index + 1):\n",
    "            batch = next(test_dl)\n",
    "        metadata = dm.metadata\n",
    "        return batch, metadata\n",
    "    else:\n",
    "        test_dl = dm.test_dataloader()\n",
    "        batch = next(iter(test_dl))\n",
    "        metadata = dm.metadata\n",
    "        return batch, metadata\n",
    "    metadata = dm.metadata\n",
    "    return batch, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94afc8-c507-41b8-a3be-dd130ff90c72",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d71514-47b0-447b-899b-5aef44c38bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(model, batch):\n",
    "    with torch.no_grad():\n",
    "        image = batch[\"pixels\"]\n",
    "        outputs = model(image)\n",
    "    outputs = F.interpolate(\n",
    "        outputs, size=(CHIP_SIZE, CHIP_SIZE), mode=\"bilinear\", align_corners=False\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64735f-70b1-4d05-acd9-2a0812545cfa",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69561e-b7ab-4f4d-b426-2d0cccc949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(batch, outputs, metadata):\n",
    "    prob_mask = outputs.cpu().numpy()\n",
    "    pred_mask = (prob_mask > 0.5).astype(float)\n",
    "    labels = batch[\"label\"].detach().cpu().numpy()\n",
    "    pixels = batch[\"pixels\"].detach().cpu().numpy()\n",
    "\n",
    "    # normalize and clip the image ranges\n",
    "    pixels = (pixels - pixels.min()) / (pixels.max() - pixels.min())\n",
    "    pixels = np.clip(pixels, 0, 1)\n",
    "\n",
    "    images = pixels[0]\n",
    "    labels = labels[0]\n",
    "    prob_mask = prob_mask[0]\n",
    "    pred_mask = pred_mask[0].astype(float)\n",
    "\n",
    "    images = images.transpose((1,2,0))\n",
    "    prob_mask = prob_mask.transpose((1,2,0))\n",
    "    pred_mask = pred_mask.transpose((1,2,0)).astype(float)\n",
    "\n",
    "    return images, labels, prob_mask, pred_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86d23c-eca7-458a-99ef-fff4534b927e",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b1925-be0f-47a5-bbb9-c642c3f04afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(images, labels, probas, preds):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 6))\n",
    "\n",
    "    # Plot the image\n",
    "    axes[0].imshow(images)\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].set_title(\"Image\", fontsize=12)\n",
    "\n",
    "    # Plot the actual segmentation\n",
    "    axes[1].imshow(labels, vmin=0, vmax=1)\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].set_title(\"Actual\", fontsize=12)\n",
    "\n",
    "    # Plot the predicted segmentation\n",
    "    axes[2].imshow(preds, vmin=0, vmax=1)\n",
    "    axes[2].axis(\"off\")\n",
    "    axes[2].set_title(\"Pred\", fontsize=12)\n",
    "\n",
    "    # Plot the predicted segmentation\n",
    "    axes[3].imshow(probas, vmin=0, vmax=1)\n",
    "    axes[3].axis(\"off\")\n",
    "    axes[3].set_title(\"Proba\", fontsize=12)\n",
    "\n",
    "    # Plot the plot_pred_vs_true_mask\n",
    "    plot_pred_vs_true_mask(images, labels, preds.squeeze(), ax=axes[4], add_legend=False)\n",
    "    axes[4].set_title(\"Pred vs True\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9b66b-ea25-4697-83be-776abb40db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = get_model(MINESEG_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0eac75",
   "metadata": {},
   "source": [
    "## Plot example predictions\n",
    "\n",
    "7 Random chips, including their ground truth and predicted masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chips = os.listdir(TEST_CHIP_DIR)\n",
    "test_chips_wo_miningarea = [chip for chip in test_chips if \"nominearea\" in chip]\n",
    "test_chips_w_miningarea = [chip for chip in test_chips if \"nominearea\" not in chip]\n",
    "indices_wo_miningarea = [test_chips.index(chip) for chip in test_chips_wo_miningarea]\n",
    "indices_w_miningarea = [test_chips.index(chip) for chip in test_chips_w_miningarea]\n",
    "\n",
    "print(f\"Number of test chips: {len(test_chips)}\")\n",
    "print(f\"Number of test chips without mining area: {len(test_chips_wo_miningarea)}\")\n",
    "print(f\"Number of test chips with mining area: {len(test_chips_w_miningarea)}\")\n",
    "\n",
    "# take a sample of the chips with mining area \n",
    "random.seed(42)\n",
    "sample_indices_w_ma = random.sample(indices_w_miningarea, 4)\n",
    "print(f\"Sample indices of chips with mining area: {sample_indices_w_ma}\")\n",
    "\n",
    "sample_indices_wo_ma = random.sample(indices_wo_miningarea, 4)\n",
    "print(f\"Sample indices of chips without mining area: {sample_indices_wo_ma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07a050-c55d-4392-9461-a16afdb65f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_predictions(sample_indices):\n",
    "    for index in sample_indices:\n",
    "\n",
    "        # Get data\n",
    "        batch, metadata = get_data(\n",
    "            TRAIN_CHIP_DIR,\n",
    "            TRAIN_LABEL_DIR,\n",
    "            VAL_CHIP_DIR,\n",
    "            VAL_LABEL_DIR,\n",
    "            TEST_CHIP_DIR,\n",
    "            TEST_LABEL_DIR,\n",
    "            METADATA_PATH,\n",
    "            BATCH_SIZE,\n",
    "            NUM_WORKERS,\n",
    "            PLATFORM,\n",
    "            data_augmentation=False,\n",
    "            index=index\n",
    "        )\n",
    "\n",
    "        # Move batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "        # Run prediction\n",
    "        outputs = run_prediction(model, batch)\n",
    "\n",
    "        # Post-process the results\n",
    "        images, labels, probas, preds = post_process(batch, outputs, metadata)\n",
    "\n",
    "        # Plot the predictions\n",
    "        plot_predictions(images, labels, probas, preds)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6281f",
   "metadata": {},
   "source": [
    "Predictions on chips with mining area: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_predictions(sample_indices_w_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37790c07",
   "metadata": {},
   "source": [
    "Predictions on chips without mining area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae749c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_predictions(sample_indices_wo_ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6902c855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plot_pred_vs_true_mask(images, labels, preds.squeeze(), ax=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5cb06",
   "metadata": {},
   "source": [
    "## Metric calculation on test dataset\n",
    "\n",
    "To calculate the metric on the test dataset, we need to predict the masks for all the chips in the test dataset and then calculate an aggregate metric.\n",
    "\n",
    "\n",
    "By default, the test set includes only chips with mining area. However, this will cover differently large areas of the actual tiles. Therefore, we will also calculate the metric on the full test set, which includes all chips from the test tiles. This makes models for different chip sizes comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4de7e",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, test_chips, testset_batch_size):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for evaluating the performance of a model on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        test_chips (list): A list of test chip file names.\n",
    "        testset_batch_size (int): The batch size for processing the test dataset.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the calculated metrics for each image in the test dataset.\n",
    "            The DataFrame has the following columns: 'image', 'file_name', 'iou', 'f1', 'accuracy', 'recall', 'precision'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a DataFrame to store the results\n",
    "    results = pd.DataFrame(columns=['image', \"file_name\", 'iou', 'f1', 'accuracy', 'recall', 'precision'])\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = int(np.ceil(len(test_chips) / testset_batch_size))\n",
    "\n",
    "    # Move model to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    for i in tqdm(range(num_batches)):\n",
    "\n",
    "        file_names = test_chips[i * testset_batch_size : (i + 1) * testset_batch_size]\n",
    "\n",
    "        # Get data\n",
    "        batch, metadata = get_data(\n",
    "            TRAIN_CHIP_DIR,\n",
    "            TRAIN_LABEL_DIR,\n",
    "            VAL_CHIP_DIR,\n",
    "            VAL_LABEL_DIR,\n",
    "            TEST_CHIP_DIR,\n",
    "            TEST_LABEL_DIR,\n",
    "            METADATA_PATH,\n",
    "            testset_batch_size,\n",
    "            NUM_WORKERS,\n",
    "            PLATFORM,\n",
    "            data_augmentation=False,\n",
    "            index=i  # Get the next batch of images\n",
    "        )\n",
    "\n",
    "        # Move batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "        # Run prediction\n",
    "        outputs = run_prediction(model, batch)\n",
    "\n",
    "        labels = batch[\"label\"].detach().cpu().numpy()\n",
    "        preds = outputs.detach().cpu().numpy()\n",
    "        preds = (preds > 0.5).astype(float)\n",
    "\n",
    "        # Calculate the metrics for each image in the batch\n",
    "        for j in range(len(file_names)):\n",
    "            iou = jaccard_score(labels[j].flatten(), preds[j].flatten())\n",
    "            f1 = f1_score(labels[j].flatten(), preds[j].flatten())\n",
    "            accuracy = accuracy_score(labels[j].flatten(), preds[j].flatten())\n",
    "            recall = recall_score(labels[j].flatten(), preds[j].flatten())\n",
    "            precision = precision_score(labels[j].flatten(), preds[j].flatten())\n",
    "\n",
    "            # Add the results to the DataFrame\n",
    "            results = pd.concat(\n",
    "                [results, \n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        'image': [i * testset_batch_size + j], \n",
    "                        'file_name': [file_names[j]],\n",
    "                        'iou': [iou], \n",
    "                        'f1': [f1], \n",
    "                        'accuracy': [accuracy], \n",
    "                        'recall': [recall], \n",
    "                        'precision': [precision]\n",
    "                    })])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfd1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_and_save(results, output_dir):\n",
    "    # load the processed dataset\n",
    "    tiles = gpd.read_file(DATASET, layer=\"tiles\")\n",
    "    test_tiles = tiles[tiles[\"split\"] == \"test\"]\n",
    "\n",
    "    # extract the tile_id from the file_name\n",
    "    results[\"tile_id\"] = results[\"file_name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "    # save as csv in the reports folder\n",
    "    output_path = output_dir + \"testset_metrics_per_chip.csv\"\n",
    "    results.to_csv(output_path, index=False)\n",
    "    print(f\"Saved per chip metrics to {output_path}\")\n",
    "\n",
    "    # Select only the numeric columns\n",
    "    numeric_columns = results.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Add the 'tile_id' column to the numeric columns\n",
    "    numeric_columns['tile_id'] = results['tile_id']\n",
    "\n",
    "    # Group by 'tile_id' and calculate the mean of the numeric columns\n",
    "    results_grouped = numeric_columns.groupby('tile_id').mean().reset_index()\n",
    "\n",
    "    # convert tile_id to int\n",
    "    results_grouped['tile_id'] = results_grouped['tile_id'].astype(int)\n",
    "\n",
    "    # Merge the results with the test_tiles\n",
    "    test_tiles = test_tiles.merge(results_grouped, on='tile_id')\n",
    "\n",
    "    # save as csv in the reports folder\n",
    "    output_path = output_dir + \"testset_metrics_aggregated.csv\"\n",
    "    test_tiles.to_csv(output_path, index=False)\n",
    "    print(f\"Saved aggregated metrics to {output_path}\")\n",
    "\n",
    "    # calculate overall metrics\n",
    "    iou = results_grouped[\"iou\"].mean()\n",
    "    f1 = results_grouped[\"f1\"].mean()\n",
    "    accuracy = results_grouped[\"accuracy\"].mean()\n",
    "    recall = results_grouped[\"recall\"].mean()\n",
    "    precision = results_grouped[\"precision\"].mean()\n",
    "\n",
    "    # print the results\n",
    "    print(f\"Mean IoU: {iou}\")\n",
    "    print(f\"Mean F1: {f1}\")\n",
    "    print(f\"Mean Accuracy: {accuracy}\")\n",
    "    print(f\"Mean Recall: {recall}\")\n",
    "    print(f\"Mean Precision: {precision}\")\n",
    "\n",
    "    return test_tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grouped_metrics(results_grouped):\n",
    "    # Metrics per mine type\n",
    "    print(\"Metrics per mine type (minetype1):\")\n",
    "    print(results_grouped.groupby(\"minetype1\")[[\"iou\", \"f1\", \"accuracy\", \"recall\", \"precision\"]].mean())\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    # Metrics per mine type\n",
    "    print(\"Metrics per mine type (minetype2):\")\n",
    "    print(results_grouped.groupby(\"minetype2\")[[\"iou\", \"f1\", \"accuracy\", \"recall\", \"precision\"]].mean())\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    # Metrics per dataset\n",
    "    print(\"Metrics per dataset (preferred_dataset):\")\n",
    "    print(results_grouped.groupby(\"preferred_dataset\")[[\"iou\", \"f1\", \"accuracy\", \"recall\", \"precision\"]].mean())\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    # print a histogram of the iou scores\n",
    "    print(\"Histogram of IoU scores:\")\n",
    "    results_grouped[\"iou\"].hist(bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921efa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iou_on_test_tiles(results_grouped):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 25))\n",
    "\n",
    "    # Assuming test_tiles is your GeoDataFrame\n",
    "    results_grouped = results_grouped.set_crs(epsg=4326)\n",
    "\n",
    "    # Convert the GeoDataFrame to Web Mercator projection (EPSG:3857)\n",
    "    results_grouped = results_grouped.to_crs(epsg=3857)\n",
    "\n",
    "    # Buffer the polygons to increase their size\n",
    "    results_grouped['geometry'] = results_grouped.geometry.buffer(200000)\n",
    "\n",
    "    # Plot the GeoDataFrame\n",
    "    results_grouped.plot(column='iou', cmap='RdYlGn', linewidth=0.8, ax=ax, edgecolor='0.8', alpha=1)\n",
    "\n",
    "    # Add a basemap\n",
    "    ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title('IoU on test tiles', fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "    # Create a colorbar as a legend\n",
    "    sm = plt.cm.ScalarMappable(cmap='RdYlGn', norm=plt.Normalize(vmin=min(results_grouped['iou']), vmax=max(results_grouped['iou'])))\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=ax, orientation='horizontal', fraction=0.03, pad=0.04)\n",
    "    cbar.ax.tick_params(labelsize=8)  # set the size of the colorbar labels\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919bed0",
   "metadata": {},
   "source": [
    "Create the new directory for the report and the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = root + f\"/reports/cnn/{str(CHIP_SIZE) + '_' + ''.join(model_name.split('.')[:-1])}\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving results to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbe498",
   "metadata": {},
   "source": [
    "### On the test set chips (mining area chips only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1e9ad",
   "metadata": {},
   "source": [
    "Define list of chips: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in the test chip directory\n",
    "test_chips = os.listdir(TEST_CHIP_DIR)\n",
    "\n",
    "# filter out only files that do not contain the string nominearea\n",
    "test_chips = [chip for chip in test_chips if \"nominearea\" not in chip]\n",
    "\n",
    "print(f\"Number of test chips: {len(test_chips)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a129883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optional: Load the aggregated metrics that were previously computed and saved. \n",
    "\n",
    "# # load the csv file with the aggregated metrics\n",
    "# test_tiles = pd.read_csv(root + \"/reports/cnn_testset_metrics_aggregated_2024-09-06_13.csv\")\n",
    "# test_tiles['geometry'] = test_tiles['geometry'].apply(loads)\n",
    "\n",
    "# # convert to gdf\n",
    "# test_tiles = gpd.GeoDataFrame(test_tiles, geometry=test_tiles[\"geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333edc9",
   "metadata": {},
   "source": [
    "Calculate the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_metrics(model, test_chips, TESTSET_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6caca4",
   "metadata": {},
   "source": [
    "Aggregate metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc2b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped = calculate_metrics_and_save(results, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e8689",
   "metadata": {},
   "source": [
    "Grouped metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grouped_metrics(results_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b4521b",
   "metadata": {},
   "source": [
    "IoU per tile on the map: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a577430",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_on_test_tiles(results_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399ae3c",
   "metadata": {},
   "source": [
    "### On the whole test set tile (including chips without mining area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d7f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in the test chip directory\n",
    "test_chips = os.listdir(TEST_CHIP_DIR)\n",
    "\n",
    "print(f\"Number of test chips: {len(test_chips)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3878e",
   "metadata": {},
   "source": [
    "Calculate the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf624ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_metrics(model, test_chips, TESTSET_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c43f12",
   "metadata": {},
   "source": [
    "Aggregate metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_grouped = calculate_metrics_and_save(results, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eba4ff",
   "metadata": {},
   "source": [
    "Grouped metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc152aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grouped_metrics(results_grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a3458",
   "metadata": {},
   "source": [
    "IoU per tile on the map: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_iou_on_test_tiles(results_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = root + \"/notebooks/cnn/cnn_inference.ipynb\"\n",
    "output_dir_report = output_dir + \"/\"\n",
    "\n",
    "# export the notebook to reports\n",
    "!jupyter nbconvert --to html $nb_path --output-dir=$output_dir_report --output=\"CNN_testset_evaluation.html\" --no-input --no-prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

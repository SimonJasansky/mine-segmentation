{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9044cc58",
   "metadata": {},
   "source": [
    "# Inference and Test Set Metric Calculation\n",
    "\n",
    "This notebook is used for both CNN and Clay model inference on the testset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877d6b9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d69a7d-5f0e-453a-8a7d-8ef4b100e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set working directory to root \n",
    "import os\n",
    "os.chdir(\"../../\")\n",
    "root = os.getcwd()\n",
    "root = root + \"/workspaces/mine-segmentation\" # for lightning studios\n",
    "print(f\"Root directory: {root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34608fe0-9c89-4b39-b0b7-59d74efafdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "# import leafmap\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "# from einops import rearrange\n",
    "# from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "# from shapely.wkt import loads\n",
    "# import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from src.models.datamodule import MineDataModule\n",
    "from src.models.cnn.model import MineSegmentorCNN\n",
    "from src.models.clay.segment.model import MineSegmentor\n",
    "\n",
    "\n",
    "from src.visualization.visualization_funcs import plot_pred_vs_true_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873272f-89e7-48de-9115-7c9d21b62c1f",
   "metadata": {},
   "source": [
    "#### Define paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea85c6-5086-42b2-b032-489890554d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2048 model\n",
    "# MINESEG_CHECKPOINT_PATH = (\"models/cnn/2048_mineseg-cnn_epoch-08_val-iou-0.5017.ckpt\")\n",
    "# CHIP_SIZE = 2048\n",
    "# TESTSET_BATCH_SIZE = 1\n",
    "# TRAIN_CHIP_DIR = \"data/processed/chips/npy/2048/train/chips/\"\n",
    "# TRAIN_LABEL_DIR = \" data/processed/chips/npy/2048/train/labels/\"\n",
    "# VAL_CHIP_DIR = \"data/processed/chips/npy/2048/val/chips/\"\n",
    "# VAL_LABEL_DIR = \"data/processed/chips/npy/2048/val/labels/\"\n",
    "# TEST_CHIP_DIR = \"data/processed/chips/npy/2048/test/chips/\"\n",
    "# TEST_LABEL_DIR = \"data/processed/chips/npy/2048/test/labels/\"\n",
    "# METADATA_PATH = \"configs/cnn/cnn_segment_metadata.yaml\"\n",
    "# CLAY = False\n",
    "\n",
    "\n",
    "# # 1024 model\n",
    "# MINESEG_CHECKPOINT_PATH = (\"models/cnn/1024_mineseg-cnn_epoch-15_val-iou-0.5290.ckpt\")\n",
    "# CHIP_SIZE = 1024\n",
    "# TESTSET_BATCH_SIZE = 4\n",
    "# TRAIN_CHIP_DIR = \"data/processed/chips/npy/1024/train/chips/\"\n",
    "# TRAIN_LABEL_DIR = \" data/processed/chips/npy/1024/train/labels/\"\n",
    "# VAL_CHIP_DIR = \"data/processed/chips/npy/1024/val/chips/\"\n",
    "# VAL_LABEL_DIR = \"data/processed/chips/npy/1024/val/labels/\"\n",
    "# TEST_CHIP_DIR = \"data/processed/chips/npy/1024/test/chips/\"\n",
    "# TEST_LABEL_DIR = \"data/processed/chips/npy/1024/test/labels/\"\n",
    "# METADATA_PATH = \"configs/cnn/cnn_segment_metadata.yaml\"\n",
    "# CLAY = False\n",
    "\n",
    "\n",
    "# 512 model\n",
    "MINESEG_CHECKPOINT_PATH = (\"models/cnn/mineseg-cnn_epoch-16_val-iou-0.5731.ckpt\")\n",
    "CHIP_SIZE = 512\n",
    "TESTSET_BATCH_SIZE = 16 # 32 for L4, 16 for PC GPU\n",
    "TRAIN_CHIP_DIR = \"data/processed/chips/npy/512/train/chips/\"\n",
    "TRAIN_LABEL_DIR = \"data/processed/chips/npy/512/train/labels/\"\n",
    "VAL_CHIP_DIR = \"data/processed/chips/npy/512/val/chips/\"\n",
    "VAL_LABEL_DIR = \"data/processed/chips/npy/512/val/labels/\"\n",
    "TEST_CHIP_DIR = \"data/processed/chips/npy/512/test/chips/\"\n",
    "TEST_LABEL_DIR = \"data/processed/chips/npy/512/test/labels/\"\n",
    "TEST_LABEL_VALIDATED_DIR = \"data/processed/chips/npy/512/validated/test/labels/\"\n",
    "METADATA_PATH = \"configs/cnn/cnn_segment_metadata.yaml\"\n",
    "CLAY = False\n",
    "\n",
    "\n",
    "# # CLAY model\n",
    "# MINESEG_CHECKPOINT_PATH = \"models/clay/mineseg-clay-segment_epoch-00_val-iou-0.3155.ckpt\"\n",
    "# CLAY_CHECKPOINT_PATH = \"models/clay-v1-base.ckpt\"\n",
    "# CHIP_SIZE = 512\n",
    "# TESTSET_BATCH_SIZE = 1\n",
    "# TRAIN_CHIP_DIR = \"data/processed/chips/npy/512/train/chips/\"\n",
    "# TRAIN_LABEL_DIR = \"data/processed/chips/npy/512/train/labels/\"\n",
    "# VAL_CHIP_DIR = \"data/processed/chips/npy/512/val/chips/\"\n",
    "# VAL_LABEL_DIR = \"data/processed/chips/npy/512/val/labels/\"\n",
    "# TEST_CHIP_DIR = \"data/processed/chips/npy/512/test/chips/\"\n",
    "# TEST_LABEL_DIR = \"data/processed/chips/npy/512/test/labels/\"\n",
    "# METADATA_PATH = \"configs/clay/clay_segment_metadata.yaml\"\n",
    "# CLAY = True\n",
    "\n",
    "\n",
    "# general setup\n",
    "DATASET = \"data/processed/mining_tiles_with_masks_and_bounding_boxes.gpkg\"\n",
    "BATCH_SIZE = 1\n",
    "if torch.cuda.is_available():\n",
    "    NUM_WORKERS = 16\n",
    "else:\n",
    "    NUM_WORKERS = 4\n",
    "PLATFORM = \"sentinel-2-l2a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLAY:\n",
    "    CLAY_CHECKPOINT_PATH = root + \"/\" + CLAY_CHECKPOINT_PATH\n",
    "\n",
    "MINESEG_CHECKPOINT_PATH = root + \"/\" + MINESEG_CHECKPOINT_PATH\n",
    "METADATA_PATH = root + \"/\" + METADATA_PATH\n",
    "TRAIN_CHIP_DIR = root +  \"/\" + TRAIN_CHIP_DIR\n",
    "TRAIN_LABEL_DIR = root + \"/\" + TRAIN_LABEL_DIR\n",
    "VAL_CHIP_DIR = root + \"/\" + VAL_CHIP_DIR\n",
    "VAL_LABEL_DIR = root + \"/\" + VAL_LABEL_DIR\n",
    "TEST_CHIP_DIR = root + \"/\" + TEST_CHIP_DIR\n",
    "TEST_LABEL_DIR = root + \"/\" + TEST_LABEL_DIR\n",
    "DATASET = root + \"/\" + DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MINESEG_CHECKPOINT_PATH.split(\"/\")[-1]\n",
    "print(f\"Using model {model_name}\")\n",
    "print(f\"Using chip size {CHIP_SIZE}\")\n",
    "print(f\"Using test chip dir {TEST_CHIP_DIR}\")\n",
    "print(f\"Using test label dir {TEST_LABEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc278db5-e241-4763-8f33-bdeb5b0f81fc",
   "metadata": {},
   "source": [
    "#### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0da577-f3e5-485a-bbc5-a3ff7367e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLAY:\n",
    "    def get_model(mineseg_checkpoint_path, clay_checkpoint_path, metadata_path):\n",
    "        model = MineSegmentor.load_from_checkpoint(\n",
    "            checkpoint_path=mineseg_checkpoint_path,\n",
    "            metadata_path=metadata_path,\n",
    "            ckpt_path=clay_checkpoint_path,\n",
    "        )\n",
    "        model.eval()\n",
    "        return model\n",
    "else: \n",
    "    def get_model(checkpoint_path: str) -> MineSegmentorCNN:\n",
    "        # check if gpu is available\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        map_location=torch.device(device)\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
    "        model_config = checkpoint[\"hyper_parameters\"]\n",
    "        model = MineSegmentorCNN.load_from_checkpoint(checkpoint_path, **model_config)\n",
    "        model.eval()\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ba7fc-f1ca-465c-be66-15edca8e8419",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3402cf0a-cb9b-47c4-a12a-bb704912edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    train_chip_dir,\n",
    "    train_label_dir,\n",
    "    val_chip_dir,\n",
    "    val_label_dir,\n",
    "    test_chip_dir,\n",
    "    test_label_dir,\n",
    "    metadata_path,\n",
    "    batch_size,\n",
    "    num_workers,\n",
    "    platform,\n",
    "    data_augmentation,\n",
    "    index=None\n",
    "):\n",
    "    dm = MineDataModule(\n",
    "        train_chip_dir=train_chip_dir,\n",
    "        train_label_dir=train_label_dir,\n",
    "        val_chip_dir=val_chip_dir,\n",
    "        val_label_dir=val_label_dir,\n",
    "        test_chip_dir=test_chip_dir,\n",
    "        test_label_dir=test_label_dir,\n",
    "        metadata_path=metadata_path,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        platform=platform,\n",
    "        data_augmentation=data_augmentation,\n",
    "    )\n",
    "    \n",
    "    dm.setup(stage=\"test\")\n",
    "    \n",
    "    if index is not None:\n",
    "        test_dl = iter(dm.test_dataloader())\n",
    "        for i in range(index + 1):\n",
    "            batch = next(test_dl)\n",
    "        metadata = dm.metadata\n",
    "        return batch, metadata\n",
    "    else:\n",
    "        test_dl = dm.test_dataloader()\n",
    "        batch = next(iter(test_dl))\n",
    "        metadata = dm.metadata\n",
    "        return batch, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94afc8-c507-41b8-a3be-dd130ff90c72",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d71514-47b0-447b-899b-5aef44c38bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(model, batch, is_clay=False):\n",
    "    with torch.no_grad():\n",
    "        if is_clay:\n",
    "            image=batch\n",
    "        else:\n",
    "            image = batch[\"pixels\"]\n",
    "        outputs = model(image)\n",
    "    outputs = F.interpolate(\n",
    "        outputs, size=(CHIP_SIZE, CHIP_SIZE), mode=\"bilinear\", align_corners=False\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64735f-70b1-4d05-acd9-2a0812545cfa",
   "metadata": {},
   "source": [
    "#### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69561e-b7ab-4f4d-b426-2d0cccc949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(batch, outputs, metadata, index=0):\n",
    "    \n",
    "    outputs = outputs.sigmoid()\n",
    "    prob_mask = outputs.cpu().numpy()\n",
    "    pred_mask = (prob_mask > 0.5).astype(float)\n",
    "    labels = batch[\"label\"].detach().cpu().numpy()\n",
    "    pixels = batch[\"pixels\"].detach().cpu().numpy()\n",
    "\n",
    "    # normalize and clip the image ranges\n",
    "    pixels = (pixels - pixels.min()) / (pixels.max() - pixels.min())\n",
    "    pixels = np.clip(pixels, 0, 1)\n",
    "\n",
    "    images = pixels[index]\n",
    "    labels = labels[index]\n",
    "    prob_mask = prob_mask[index]\n",
    "    pred_mask = pred_mask[index].astype(float)\n",
    "\n",
    "    images = images.transpose((1,2,0))\n",
    "    prob_mask = prob_mask.transpose((1,2,0))\n",
    "    pred_mask = pred_mask.transpose((1,2,0)).astype(float)\n",
    "\n",
    "    # normalize the probablity mask\n",
    "    # prob_mask = (prob_mask - prob_mask.min()) / (prob_mask.max() - prob_mask.min())\n",
    "\n",
    "    return images, labels, prob_mask, pred_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86d23c-eca7-458a-99ef-fff4534b927e",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b1925-be0f-47a5-bbb9-c642c3f04afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(images, labels, probas, preds):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 6))\n",
    "\n",
    "    # Plot the image\n",
    "    axes[0].imshow(images)\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].set_title(\"Image\", fontsize=12)\n",
    "\n",
    "    # Plot the actual segmentation\n",
    "    axes[1].imshow(labels, vmin=0, vmax=1)\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].set_title(\"Actual\", fontsize=12)\n",
    "\n",
    "    # Plot the predicted segmentation\n",
    "    axes[2].imshow(preds, vmin=0, vmax=1)\n",
    "    axes[2].axis(\"off\")\n",
    "    axes[2].set_title(\"Pred\", fontsize=12)\n",
    "\n",
    "    # Plot the predicted segmentation\n",
    "    axes[3].imshow(probas, vmin=0, vmax=1)\n",
    "    axes[3].axis(\"off\")\n",
    "    axes[3].set_title(\"Proba\", fontsize=12)\n",
    "\n",
    "    # Plot the plot_pred_vs_true_mask\n",
    "    plot_pred_vs_true_mask(images, labels, preds.squeeze(), ax=axes[4], add_legend=False)\n",
    "    axes[4].set_title(\"Pred vs True\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9b66b-ea25-4697-83be-776abb40db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "if CLAY:\n",
    "    model = get_model(MINESEG_CHECKPOINT_PATH, CLAY_CHECKPOINT_PATH, METADATA_PATH)\n",
    "else:\n",
    "    model = get_model(MINESEG_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0eac75",
   "metadata": {},
   "source": [
    "## Plot example predictions\n",
    "\n",
    "Plot 4 chips with mining area, and 4 without mining area. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chips = os.listdir(TEST_CHIP_DIR)\n",
    "test_chips_indices = [test_chips.index(chip) for chip in test_chips]\n",
    "test_chips_wo_miningarea = [chip for chip in test_chips if \"nominearea\" in chip]\n",
    "test_chips_w_miningarea = [chip for chip in test_chips if \"nominearea\" not in chip]\n",
    "indices_wo_miningarea = [test_chips.index(chip) for chip in test_chips_wo_miningarea]\n",
    "indices_w_miningarea = [test_chips.index(chip) for chip in test_chips_w_miningarea]\n",
    "\n",
    "print(f\"Number of all test chips: {len(test_chips)}\")\n",
    "print(f\"Number of test chips without mining area: {len(test_chips_wo_miningarea)}\")\n",
    "print(f\"Number of test chips with mining area: {len(test_chips_w_miningarea)}\")\n",
    "\n",
    "# take a sample of the chips with mining area \n",
    "# random.seed(42)\n",
    "sample_indices_w_ma = random.sample(indices_w_miningarea, 4)\n",
    "print(f\"Sample indices of chips with mining area: {sample_indices_w_ma}\")\n",
    "if not CHIP_SIZE == 2048:\n",
    "    sample_indices_wo_ma = random.sample(indices_wo_miningarea, 4)\n",
    "    print(f\"Sample indices of chips without mining area: {sample_indices_wo_ma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07a050-c55d-4392-9461-a16afdb65f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_predictions(sample_indices):\n",
    "    for index in sample_indices:\n",
    "\n",
    "        # Get data\n",
    "        batch, metadata = get_data(\n",
    "            TRAIN_CHIP_DIR,\n",
    "            TRAIN_LABEL_DIR,\n",
    "            VAL_CHIP_DIR,\n",
    "            VAL_LABEL_DIR,\n",
    "            TEST_CHIP_DIR,\n",
    "            TEST_LABEL_DIR,\n",
    "            METADATA_PATH,\n",
    "            BATCH_SIZE,\n",
    "            NUM_WORKERS,\n",
    "            PLATFORM,\n",
    "            data_augmentation=False,\n",
    "            index=index\n",
    "        )\n",
    "\n",
    "        # Move batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "        # Run prediction\n",
    "        outputs = run_prediction(model, batch, is_clay=CLAY)\n",
    "\n",
    "        # Post-process the results\n",
    "        images, labels, probas, preds = post_process(batch, outputs, metadata)\n",
    "\n",
    "        # Plot the predictions\n",
    "        plot_predictions(images, labels, probas, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6281f",
   "metadata": {},
   "source": [
    "#### Predictions on chips with mining area: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_predictions(sample_indices_w_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37790c07",
   "metadata": {},
   "source": [
    "#### Predictions on chips without mining area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae749c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not CHIP_SIZE == 2048:\n",
    "    plot_sample_predictions(sample_indices_wo_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5cb06",
   "metadata": {},
   "source": [
    "## Metric calculation on test dataset\n",
    "\n",
    "To calculate the metric on the test dataset, we need to predict the masks for all the chips in the test dataset and then calculate an aggregate metric.\n",
    "\n",
    "\n",
    "By default, the test set includes only chips with mining area. However, this will cover differently large areas of the actual tiles. Therefore, we will also calculate the metric on the full test set, which includes all chips from the test tiles. This makes models for different chip sizes comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4de7e",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_to_temp_directory(tile_id, test_chip_dir, test_label_dir, temp_chip_dir, temp_label_dir):\n",
    "    \"\"\"\n",
    "    Copies files with the specified tile_id from the test directories to the temporary directories.\n",
    "\n",
    "    Args:\n",
    "        tile_id (int): The tile ID to filter the files.\n",
    "        test_chip_dir (str): The directory containing the test chip files.\n",
    "        test_label_dir (str): The directory containing the test label files.\n",
    "        temp_chip_dir (str): The temporary directory to copy the chip files.\n",
    "        temp_label_dir (str): The temporary directory to copy the label files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the temporary chip directory, temporary label directory, and the last copied chip file.\n",
    "\n",
    "    \"\"\"\n",
    "    # Remove any existing files in the directories\n",
    "    for chip in os.listdir(temp_chip_dir):\n",
    "        os.remove(os.path.join(temp_chip_dir, chip))\n",
    "    for mask in os.listdir(temp_label_dir):\n",
    "        os.remove(os.path.join(temp_label_dir, mask))\n",
    "\n",
    "    filenames = []\n",
    "    # Move chip files with the specified tile_id to the temp directory\n",
    "    for chip in os.listdir(test_chip_dir):\n",
    "        if int(chip.split(\"_\")[0]) == tile_id:\n",
    "            shutil.copy(os.path.join(test_chip_dir, chip), os.path.join(temp_chip_dir, chip))\n",
    "            # get file name without extension and nominearea addition\n",
    "            filename = chip.split(\"_\")[0:7]\n",
    "            filename = \"_\".join(filename)\n",
    "            filenames.append(filename)\n",
    "\n",
    "    # Move mask files with the specified tile_id to the temp directory\n",
    "    for mask in os.listdir(test_label_dir):\n",
    "        if int(mask.split(\"_\")[0]) == tile_id:\n",
    "            shutil.copy(os.path.join(test_label_dir, mask), os.path.join(temp_label_dir, mask))\n",
    "            filename = mask.split(\"_\")[0:7]\n",
    "            filename = \"_\".join(filename)\n",
    "            filenames.append(filename)\n",
    "\n",
    "    # make sure that the filenames are all the same\n",
    "    filenames = list(set(filenames))\n",
    "    assert len(filenames) == 1, \"Filenames are not all the same\"\n",
    "\n",
    "    # print(f\"Number of chips: {len(os.listdir(temp_chip_dir))}\")\n",
    "    # print(f\"Number of masks: {len(os.listdir(temp_label_dir))}\")\n",
    "    # print(\"Filename: \", filename)\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(\n",
    "        model, \n",
    "        test_chip_dir, \n",
    "        test_label_dir,\n",
    "        testset_batch_size, \n",
    "        calculate_per_tile=False\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for evaluating the performance of a model on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        test_chip_dir (str): The directory containing the test dataset chips.\n",
    "        test_label_dir (str): The directory containing the test dataset labels.\n",
    "        testset_batch_size (int): The batch size for processing the test dataset.\n",
    "        calculate_per_tile (bool): If True, the metrics will be calculated for each tile in the test dataset.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the calculated metrics for each image in the test dataset.\n",
    "            The DataFrame has the following columns: 'image', 'file_name', 'iou', 'f1', 'accuracy', 'recall', 'precision'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a DataFrame to store the results\n",
    "    results = pd.DataFrame(columns=['image', \"file_name\", 'iou', 'f1', 'accuracy', 'recall', 'precision'])\n",
    "\n",
    "    # Get the list of test chips\n",
    "    test_chips = os.listdir(test_chip_dir)\n",
    "    test_masks = os.listdir(test_label_dir)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = int(np.ceil(len(test_chips) / testset_batch_size))\n",
    "\n",
    "    # Move model to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "\n",
    "    if calculate_per_tile:\n",
    "        # Create a new directory at the same level as TEST_CHIP_DIR with the name temp\n",
    "        greatgrandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(test_chip_dir)))\n",
    "        temp_dir = os.path.join(greatgrandparent_dir, \"temp\")\n",
    "        Path(os.path.join(temp_dir, \"chips\")).mkdir(parents=True, exist_ok=True)\n",
    "        Path(os.path.join(temp_dir, \"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        temp_chip_dir = os.path.join(temp_dir, \"chips/\")\n",
    "        temp_label_dir = os.path.join(temp_dir, \"labels/\")\n",
    "        print(f\"Temporary chip directory: {temp_chip_dir}\")\n",
    "        print(f\"Temporary label directory: {temp_label_dir}\")\n",
    "\n",
    "        # Get unique tile ids\n",
    "        tile_ids_chips = [chip.split(\"_\")[0] for chip in test_chips]\n",
    "        tile_ids_masks = [mask.split(\"_\")[0] for mask in test_masks]\n",
    "        unique_tile_ids_chips = set(tile_ids_chips)\n",
    "        unique_tile_ids_masks = set(tile_ids_masks)\n",
    "\n",
    "        assert unique_tile_ids_chips == unique_tile_ids_masks\n",
    "        unique_tile_ids = sorted(list(unique_tile_ids_chips))\n",
    "        print(f\"Number of unique tile ids: {len(unique_tile_ids)}\")\n",
    "        \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        \n",
    "        if calculate_per_tile:\n",
    "            tile_id = int(unique_tile_ids[i])\n",
    "\n",
    "            file_name = copy_files_to_temp_directory(\n",
    "                tile_id, \n",
    "                test_chip_dir, \n",
    "                test_label_dir, \n",
    "                temp_chip_dir, \n",
    "                temp_label_dir\n",
    "                )\n",
    "            index=0\n",
    "            chip_dir = temp_chip_dir\n",
    "            label_dir = temp_label_dir\n",
    "            file_names = [file_name]\n",
    "            assert len(file_names) == 1\n",
    "        else:\n",
    "            # Get the file names for the current batch\n",
    "            file_names = test_chips[i * testset_batch_size : (i + 1) * testset_batch_size]\n",
    "            index=i\n",
    "            chip_dir = test_chip_dir\n",
    "            label_dir = test_label_dir\n",
    "\n",
    "        # Get data\n",
    "        batch, metadata = get_data(\n",
    "            TRAIN_CHIP_DIR,\n",
    "            TRAIN_LABEL_DIR,\n",
    "            VAL_CHIP_DIR,\n",
    "            VAL_LABEL_DIR,\n",
    "            chip_dir,\n",
    "            label_dir,\n",
    "            METADATA_PATH,\n",
    "            testset_batch_size,\n",
    "            NUM_WORKERS,\n",
    "            PLATFORM,\n",
    "            data_augmentation=False,\n",
    "            index=index\n",
    "        )\n",
    "        \n",
    "        # Move batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "        # Run prediction\n",
    "        outputs = run_prediction(model, batch, is_clay=CLAY)\n",
    "\n",
    "        labels = batch[\"label\"].detach().cpu().numpy()\n",
    "        probas = outputs.sigmoid().detach().cpu().numpy()\n",
    "        preds = (probas > 0.5).astype(float)\n",
    "            \n",
    "        # Calculate the metrics for each image in the batch\n",
    "        for j in range(len(file_names)):\n",
    "\n",
    "            if calculate_per_tile:\n",
    "                true_mask = labels.flatten()\n",
    "                pred_mask = preds.flatten()\n",
    "            else:\n",
    "                true_mask = labels[j].flatten()\n",
    "                pred_mask = preds[j].flatten()\n",
    "\n",
    "            iou = jaccard_score(true_mask, pred_mask)\n",
    "            f1 = f1_score(true_mask, pred_mask)\n",
    "            accuracy = accuracy_score(true_mask, pred_mask)\n",
    "            recall = recall_score(true_mask, pred_mask)\n",
    "            precision = precision_score(true_mask, pred_mask)\n",
    "\n",
    "            # Add the results to the DataFrame\n",
    "            results = pd.concat(\n",
    "                [results, \n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        'image': [i * testset_batch_size + j], \n",
    "                        'file_name': [file_names[j]],\n",
    "                        'iou': [iou], \n",
    "                        'f1': [f1], \n",
    "                        'accuracy': [accuracy], \n",
    "                        'recall': [recall], \n",
    "                        'precision': [precision],\n",
    "                    })])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grouped_metrics(results_grouped):\n",
    "    # Metrics per mine type\n",
    "    print(\"Metrics per mine type (minetype1):\")\n",
    "    count = results_grouped.groupby(\"minetype1\")[[\"iou\"]].count()\n",
    "    count.columns = [\"count\"]\n",
    "    mean = results_grouped.groupby(\"minetype1\")[[\"iou\", \"f1\", \"accuracy\", \"recall\", \"precision\"]]\n",
    "    mean = pd.concat([count, mean.mean()], axis=1)\n",
    "    print(mean)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    print(\"Metrics per mine type (minetype2):\")\n",
    "    count = results_grouped.groupby(\"minetype2\")[[\"iou\"]].count()\n",
    "    count.columns = [\"count\"]\n",
    "    mean = results_grouped.groupby(\"minetype2\")[[\"iou\", \"f1\", \"accuracy\", \"recall\", \"precision\"]]\n",
    "    mean = pd.concat([count, mean.mean()], axis=1)\n",
    "    print(mean)\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    # Metrics per dataset\n",
    "    print(\"Metrics per dataset (preferred_dataset):\")\n",
    "    count = results_grouped.groupby(\"preferred_dataset\")[[\"iou\"]].count()\n",
    "    count.columns = [\"count\"]\n",
    "    mean = results_grouped.groupby(\"preferred_dataset\")[[\"iou\", \"f1\", \"accuracy\", \"recall\", \"precision\"]]\n",
    "    mean = pd.concat([count, mean.mean()], axis=1)\n",
    "    print(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921efa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iou_on_test_tiles(results_grouped, add_tile_id=False):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 25))\n",
    "\n",
    "    # Assuming test_tiles is your GeoDataFrame\n",
    "    results_grouped = results_grouped.set_crs(epsg=4326)\n",
    "\n",
    "    # Convert the GeoDataFrame to Web Mercator projection (EPSG:3857)\n",
    "    results_grouped = results_grouped.to_crs(epsg=3857)\n",
    "\n",
    "    plot_df = results_grouped.copy()\n",
    "\n",
    "    # Buffer the polygons to increase their size\n",
    "    plot_df['geometry'] = plot_df.geometry.buffer(170000)\n",
    "\n",
    "    # Plot the GeoDataFrame\n",
    "    plot_df.plot(column='iou', cmap='RdYlGn', linewidth=0.8, ax=ax, edgecolor='0.8', alpha=0.8)\n",
    "\n",
    "    # Add a basemap\n",
    "    ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title('IoU on test tiles', fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "    # Create a colorbar as a legend\n",
    "    sm = plt.cm.ScalarMappable(cmap='RdYlGn', norm=plt.Normalize(vmin=min(plot_df['iou']), vmax=max(plot_df['iou'])))\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=ax, orientation='horizontal', fraction=0.03, pad=0.04)\n",
    "    cbar.ax.tick_params(labelsize=8)  # set the size of the colorbar labels\n",
    "\n",
    "    if add_tile_id: \n",
    "        # Annotate each geometry with the label from the specified column\n",
    "        for idx, row in plot_df.iterrows():\n",
    "            centroid = row['geometry'].centroid\n",
    "            ax.annotate(\n",
    "                text=row[\"tile_id\"], \n",
    "                xy=(centroid.x, centroid.y), \n",
    "                xytext=(5, 5),  # shift annotation to the top right\n",
    "                textcoords='offset points', \n",
    "                horizontalalignment='center', \n",
    "                fontsize=8, \n",
    "                color='black',\n",
    "                rotation=45  # angle the text by 45 degrees\n",
    "            )\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919bed0",
   "metadata": {},
   "source": [
    "Create the new directory for the report and the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLAY:\n",
    "    output_dir = root + f\"/reports/clay/{''.join(model_name.split('.')[:-1])}\"\n",
    "else:\n",
    "    output_dir = root + f\"/reports/cnn/{''.join(model_name.split('.')[:-1])}\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving results to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbe498",
   "metadata": {},
   "source": [
    "### 1. On the test set chips (mining area chips only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1e9ad",
   "metadata": {},
   "source": [
    "Copy Chips and Labels with mining area to a separate directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the test chip directory\n",
    "test_chips = os.listdir(TEST_CHIP_DIR)\n",
    "test_masks = os.listdir(TEST_LABEL_DIR)\n",
    "\n",
    "# Create a new directory at the same level as TEST_CHIP_DIR with the name test_minearea_only\n",
    "greatgrandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(TEST_CHIP_DIR)))\n",
    "test_minearea_only_dir = os.path.join(greatgrandparent_dir, \"test_minearea_only\")\n",
    "Path(os.path.join(test_minearea_only_dir, \"chips\")).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.path.join(test_minearea_only_dir, \"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "minearea_chip_dir = os.path.join(test_minearea_only_dir, \"chips/\")\n",
    "minearea_label_dir = os.path.join(test_minearea_only_dir, \"labels/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ce96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move files containing \"nominearea\" to the respective directories\n",
    "for chip in tqdm(test_chips):\n",
    "    if \"nominearea\" not in chip:\n",
    "        # Move chip file\n",
    "        shutil.copy(os.path.join(TEST_CHIP_DIR, chip), os.path.join(minearea_chip_dir, chip))\n",
    "\n",
    "# Move files containing \"nominearea\" to the respective directories\n",
    "for mask in tqdm(test_masks):\n",
    "    if \"nominearea\" not in mask:\n",
    "        # Move mask file\n",
    "        shutil.copy(os.path.join(TEST_LABEL_DIR, mask), os.path.join(minearea_label_dir, mask))\n",
    "\n",
    "print(f\"Copied chips with mining area to {minearea_chip_dir}\")\n",
    "print(f\"Copied labels with mining area to {minearea_label_dir}\")\n",
    "print(f\"Number of chips with mining area: {len(os.listdir(minearea_chip_dir))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333edc9",
   "metadata": {},
   "source": [
    "Calculate the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = calculate_metrics(model, minearea_chip_dir, minearea_label_dir, TESTSET_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6caca4",
   "metadata": {},
   "source": [
    "**Metrics on the Test Set (Average over all chips with mining area):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9856fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load the processed dataset\n",
    "# tiles = gpd.read_file(DATASET, layer=\"tiles\")\n",
    "# test_tiles = tiles[tiles[\"split\"] == \"test\"]\n",
    "\n",
    "# # extract the tile_id from the file_name\n",
    "# results[\"tile_id\"] = results[\"file_name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "# # convert tile_id to int\n",
    "# results['tile_id'] = results['tile_id'].astype(int)\n",
    "\n",
    "# # Merge the results with the test_tiles\n",
    "# results_merged = results.merge(test_tiles[[\"tile_id\", \"preferred_dataset\", \"minetype1\", \"minetype2\"]], on='tile_id')\n",
    "\n",
    "# # # save as csv in the reports folder\n",
    "# output_path = output_dir + \"/testset_metrics_per_chip.csv\"\n",
    "# results_merged.to_csv(output_path, index=False)\n",
    "# print(f\"Saved per-chip metrics to {output_path}\")\n",
    "\n",
    "# # calculate overall metrics\n",
    "# iou = results_merged[\"iou\"].mean()\n",
    "# f1 = results_merged[\"f1\"].mean()\n",
    "# accuracy = results_merged[\"accuracy\"].mean()\n",
    "# recall = results_merged[\"recall\"].mean()\n",
    "# precision = results_merged[\"precision\"].mean()\n",
    "\n",
    "# # print the results\n",
    "# print(f\"Mean IoU: {iou}\")\n",
    "# print(f\"Mean F1: {f1}\")\n",
    "# print(f\"Mean Accuracy: {accuracy}\")\n",
    "# print(f\"Mean Recall: {recall}\")\n",
    "# print(f\"Mean Precision: {precision}\")\n",
    "\n",
    "# results_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e8689",
   "metadata": {},
   "source": [
    "Grouped metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_grouped_metrics(results_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5023db31",
   "metadata": {},
   "source": [
    "Histogram of the IoU: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef499b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print a histogram of the iou scores\n",
    "# results_merged[\"iou\"].hist(bins=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66de361",
   "metadata": {},
   "source": [
    "Boxplots: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e890e86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create boxplots of the iou scores\n",
    "# results_merged.boxplot(column=[\"iou\", \"f1\", \"accuracy\", \"precision\", \"recall\"], figsize=(10, 6), rot=45, grid=False)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399ae3c",
   "metadata": {},
   "source": [
    "### 2. On the whole test set tile (including chips without mining area)\n",
    "This allows us to compare the metrics between models with different chip sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3878e",
   "metadata": {},
   "source": [
    "Calculate the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf624ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the batch size, so that one batch includes all chips for a tile\n",
    "batch_size_tile = int((2048/CHIP_SIZE)**2)\n",
    "print(f\"Batch size: {batch_size_tile}\")\n",
    "\n",
    "results = calculate_metrics(model, TEST_CHIP_DIR, TEST_LABEL_DIR, batch_size_tile, calculate_per_tile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c43f12",
   "metadata": {},
   "source": [
    "Per-Tile metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the processed dataset\n",
    "tiles = gpd.read_file(DATASET, layer=\"tiles\")\n",
    "test_tiles = tiles[tiles[\"split\"] == \"test\"]\n",
    "\n",
    "# extract the tile_id from the file_name\n",
    "results[\"tile_id\"] = results[\"file_name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "# convert tile_id to int\n",
    "results['tile_id'] = results['tile_id'].astype(int)\n",
    "\n",
    "# Merge the results with the test_tiles\n",
    "results_merged = results.merge(test_tiles[[\"tile_id\", \"preferred_dataset\", \"minetype1\", \"minetype2\", \"geometry\"]], on='tile_id')\n",
    "\n",
    "# # save as csv in the reports folder\n",
    "output_path = output_dir + \"/testset_metrics_per_tile.csv\"\n",
    "results_merged.to_csv(output_path, index=False)\n",
    "print(f\"Saved aggregated metrics to {output_path}\")\n",
    "\n",
    "# calculate overall metrics\n",
    "iou = results_merged[\"iou\"].mean()\n",
    "f1 = results_merged[\"f1\"].mean()\n",
    "accuracy = results_merged[\"accuracy\"].mean()\n",
    "recall = results_merged[\"recall\"].mean()\n",
    "precision = results_merged[\"precision\"].mean()\n",
    "\n",
    "# print the results\n",
    "print(f\"Mean IoU: {iou}\")\n",
    "print(f\"Mean F1: {f1}\")\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "print(f\"Mean Recall: {recall}\")\n",
    "print(f\"Mean Precision: {precision}\")\n",
    "results_merged[['tile_id', 'preferred_dataset', 'minetype1', 'minetype2', 'iou', 'f1', 'accuracy', 'recall', 'precision']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eba4ff",
   "metadata": {},
   "source": [
    "Grouped metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc152aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grouped_metrics(results_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e626a97",
   "metadata": {},
   "source": [
    "Histogram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9149cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a histogram of the iou scores\n",
    "results_merged[\"iou\"].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c8bbbb",
   "metadata": {},
   "source": [
    "Boxplots: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46c68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boxplots of the iou scores\n",
    "results_merged.boxplot(column=[\"iou\", \"f1\", \"accuracy\", \"precision\", \"recall\"], figsize=(10, 6), rot=45, grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a3458",
   "metadata": {},
   "source": [
    "IoU per tile on the map: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_merged_gdf = gpd.GeoDataFrame(results_merged, geometry=results_merged[\"geometry\"])\n",
    "\n",
    "plot_iou_on_test_tiles(results_merged_gdf, add_tile_id = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6957212c",
   "metadata": {},
   "source": [
    "### 3. On the whole test set tile, using validated mining areas\n",
    "\n",
    "This allows us to see if the model performs better or worse on a set of validated polyons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74339243",
   "metadata": {},
   "source": [
    "Calculate the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db77fd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the batch size, so that one batch includes all chips for a tile\n",
    "batch_size_tile = int((2048/CHIP_SIZE)**2)\n",
    "print(f\"Batch size: {batch_size_tile}\")\n",
    "\n",
    "results = calculate_metrics(model, TEST_CHIP_DIR, TEST_LABEL_VALIDATED_DIR, batch_size_tile, calculate_per_tile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8dff98",
   "metadata": {},
   "source": [
    "Per-Tile metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef490b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the processed dataset\n",
    "tiles = gpd.read_file(DATASET, layer=\"tiles\")\n",
    "test_tiles = tiles[tiles[\"split\"] == \"test\"]\n",
    "\n",
    "# extract the tile_id from the file_name\n",
    "results[\"tile_id\"] = results[\"file_name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "# convert tile_id to int\n",
    "results['tile_id'] = results['tile_id'].astype(int)\n",
    "\n",
    "# Merge the results with the test_tiles\n",
    "results_merged = results.merge(test_tiles[[\"tile_id\", \"preferred_dataset\", \"minetype1\", \"minetype2\", \"geometry\"]], on='tile_id')\n",
    "\n",
    "# # save as csv in the reports folder\n",
    "output_path = output_dir + \"/testset_metrics_per_tile.csv\"\n",
    "results_merged.to_csv(output_path, index=False)\n",
    "print(f\"Saved aggregated metrics to {output_path}\")\n",
    "\n",
    "# calculate overall metrics\n",
    "iou = results_merged[\"iou\"].mean()\n",
    "f1 = results_merged[\"f1\"].mean()\n",
    "accuracy = results_merged[\"accuracy\"].mean()\n",
    "recall = results_merged[\"recall\"].mean()\n",
    "precision = results_merged[\"precision\"].mean()\n",
    "\n",
    "# print the results\n",
    "print(f\"Mean IoU: {iou}\")\n",
    "print(f\"Mean F1: {f1}\")\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "print(f\"Mean Recall: {recall}\")\n",
    "print(f\"Mean Precision: {precision}\")\n",
    "results_merged[['tile_id', 'preferred_dataset', 'minetype1', 'minetype2', 'iou', 'f1', 'accuracy', 'recall', 'precision']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f90562",
   "metadata": {},
   "source": [
    "Grouped metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b77c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grouped_metrics(results_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba170071",
   "metadata": {},
   "source": [
    "Histogram: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420d1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print a histogram of the iou scores\n",
    "results_merged[\"iou\"].hist(bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cb5588",
   "metadata": {},
   "source": [
    "Boxplots: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220eaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create boxplots of the iou scores\n",
    "results_merged.boxplot(column=[\"iou\", \"f1\", \"accuracy\", \"precision\", \"recall\"], figsize=(10, 6), rot=45, grid=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f106a0",
   "metadata": {},
   "source": [
    "IoU per tile on the map: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ee7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_merged_gdf = gpd.GeoDataFrame(results_merged, geometry=results_merged[\"geometry\"])\n",
    "\n",
    "plot_iou_on_test_tiles(results_merged_gdf, add_tile_id = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606b4f4",
   "metadata": {},
   "source": [
    "## Plot individual tiles and predictions\n",
    "You can check the dataframe above (results_merged), and indicate the index that should be displayed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4783fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new directory at the same level as TEST_CHIP_DIR with the name temp\n",
    "greatgrandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(TEST_CHIP_DIR)))\n",
    "temp_dir = os.path.join(greatgrandparent_dir, \"temp\")\n",
    "Path(os.path.join(temp_dir, \"chips\")).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.path.join(temp_dir, \"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "temp_chip_dir = os.path.join(temp_dir, \"chips/\")\n",
    "temp_label_dir = os.path.join(temp_dir, \"labels/\")\n",
    "\n",
    "batch_size_tile = int((2048/CHIP_SIZE)**2)\n",
    "print(f\"Batch size: {batch_size_tile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a97ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_id = 1100\n",
    "\n",
    "file_name = copy_files_to_temp_directory(tile_id, TEST_CHIP_DIR, TEST_LABEL_DIR, temp_chip_dir, temp_label_dir)\n",
    "print(f\"Using tile_id {tile_id}, with filename {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "batch, metadata = get_data(\n",
    "    TRAIN_CHIP_DIR,\n",
    "    TRAIN_LABEL_DIR,\n",
    "    VAL_CHIP_DIR,\n",
    "    VAL_LABEL_DIR,\n",
    "    temp_chip_dir,\n",
    "    temp_label_dir,\n",
    "    METADATA_PATH,\n",
    "    batch_size_tile,\n",
    "    NUM_WORKERS,\n",
    "    PLATFORM,\n",
    "    data_augmentation=False,\n",
    "    index=0\n",
    ")\n",
    "\n",
    "# Move batch to GPU\n",
    "if torch.cuda.is_available():\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "# Run prediction\n",
    "outputs = run_prediction(model, batch, is_clay=CLAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in the temp directory\n",
    "temp_chips = os.listdir(temp_chip_dir)\n",
    "\n",
    "# extract the chip number\n",
    "chip_num = [int(chip.split(\"_\")[9].split(\".\")[0]) for chip in temp_chips]\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "probas = []\n",
    "preds = []\n",
    "\n",
    "# Iterate over the indices in the batch\n",
    "for i in range(len(chip_num)):\n",
    "    index = chip_num.index(i)\n",
    "    # print(f\"Processing chip {temp_chips[index]}\")\n",
    "    # Post-process the results\n",
    "    image, label, proba, pred = post_process(batch, outputs, metadata, index=index)\n",
    "    images.append(image)\n",
    "    labels.append(label)\n",
    "    probas.append(proba.squeeze())\n",
    "    preds.append(pred.squeeze())\n",
    "\n",
    "# Combine the 16 images into a single 2048x2048 image\n",
    "def put_np_together(images, channels=3, chip_size=512):\n",
    "    if channels == 1:\n",
    "        big_image = np.zeros((2048, 2048))\n",
    "    elif channels == 3:\n",
    "        big_image = np.zeros((2048, 2048, 3))\n",
    "\n",
    "    # Define the chip size and the number of chips in each dimension\n",
    "    chip_size = chip_size\n",
    "    n_chips_x = 2048 // chip_size\n",
    "    n_chips_y = 2048 // chip_size\n",
    "\n",
    "    # Iterate over the 16 images and place them in the correct position\n",
    "    chip_number = 0\n",
    "    for i in range(n_chips_x):  # Iterate over columns\n",
    "        for j in range(n_chips_y):  # Iterate over rows\n",
    "            x1, y1 = j * chip_size, i * chip_size\n",
    "            x2, y2 = x1 + chip_size, y1 + chip_size\n",
    "            if channels == 1:\n",
    "                big_image[x1:x2, y1:y2] = images[chip_number]\n",
    "            else:\n",
    "                big_image[x1:x2, y1:y2, :] = images[chip_number]\n",
    "            chip_number += 1\n",
    "\n",
    "    return big_image\n",
    "\n",
    "big_image = put_np_together(images, channels=3, chip_size=CHIP_SIZE)\n",
    "big_label = put_np_together(labels, channels=1, chip_size=CHIP_SIZE)\n",
    "big_proba = put_np_together(probas, channels=1, chip_size=CHIP_SIZE)\n",
    "big_pred = put_np_together(preds, channels=1, chip_size=CHIP_SIZE)\n",
    "\n",
    "plot_predictions(big_image, big_label, big_proba, big_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f82490",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_vs_true_mask(big_image, big_label, big_pred, add_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMEMBER TO SAVE THE NOTEBOOK BEFORE RUNNING THIS CELL!!!!\n",
    "nb_path = root + \"/notebooks/cnn/cnn_inference.ipynb\"\n",
    "output_dir_report = output_dir + \"/\"\n",
    "print(f\"Exporting notebook to {output_dir_report}\")\n",
    "\n",
    "# export the notebook to reports\n",
    "!jupyter nbconvert --to html $nb_path --output-dir=$output_dir_report --output=\"CNN_testset_evaluation.html\" --no-input --no-prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

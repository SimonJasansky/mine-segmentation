{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9044cc58",
   "metadata": {},
   "source": [
    "# CNN Inference and Test Set Metric Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b877d6b9",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d69a7d-5f0e-453a-8a7d-8ef4b100e72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# set working directory to root \n",
    "import os\n",
    "os.chdir(\"../../\")\n",
    "root = os.getcwd()\n",
    "# root = root + \"/workspaces/mine-segmentation\" # for lightning studios\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34608fe0-9c89-4b39-b0b7-59d74efafdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import leafmap\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from einops import rearrange\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.metrics import jaccard_score, f1_score, accuracy_score, recall_score, precision_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import contextily as ctx\n",
    "from shapely.wkt import loads\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import random\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from src.models.datamodule import MineDataModule\n",
    "from src.models.cnn.model import MineSegmentorCNN\n",
    "\n",
    "from src.visualization.visualization_funcs import plot_pred_vs_true_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d9a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8873272f-89e7-48de-9115-7c9d21b62c1f",
   "metadata": {},
   "source": [
    "### Define paths and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ea85c6-5086-42b2-b032-489890554d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 2048 model\n",
    "# MINESEG_CHECKPOINT_PATH = (\"models/cnn/2048_mineseg-cnn_epoch-08_val-iou-0.5017.ckpt\")\n",
    "# CHIP_SIZE = 2048\n",
    "# TESTSET_BATCH_SIZE = 8\n",
    "# TRAIN_CHIP_DIR = \"data/processed/chips/npy/1024/train/chips/\"\n",
    "# TRAIN_LABEL_DIR = \" data/processed/chips/npy/1024/train/labels/\"\n",
    "# VAL_CHIP_DIR = \"data/processed/chips/npy/1024/val/chips/\"\n",
    "# VAL_LABEL_DIR = \"data/processed/chips/npy/1024/val/labels/\"\n",
    "# TEST_CHIP_DIR = \"data/processed/chips/npy/1024/test/chips/\"\n",
    "# TEST_LABEL_DIR = \"data/processed/chips/npy/1024/test/labels/\"\n",
    "\n",
    "# # 1024 model\n",
    "# MINESEG_CHECKPOINT_PATH = (\"models/cnn/1024_mineseg-cnn_epoch-15_val-iou-0.5290.ckpt\")\n",
    "# CHIP_SIZE = 1024\n",
    "# TESTSET_BATCH_SIZE = 16\n",
    "# TRAIN_CHIP_DIR = \"data/processed/chips/npy/1024/train/chips/\"\n",
    "# TRAIN_LABEL_DIR = \" data/processed/chips/npy/1024/train/labels/\"\n",
    "# VAL_CHIP_DIR = \"data/processed/chips/npy/1024/val/chips/\"\n",
    "# VAL_LABEL_DIR = \"data/processed/chips/npy/1024/val/labels/\"\n",
    "# TEST_CHIP_DIR = \"data/processed/chips/npy/1024/test/chips/\"\n",
    "# TEST_LABEL_DIR = \"data/processed/chips/npy/1024/test/labels/\"\n",
    "\n",
    "# 512 model\n",
    "MINESEG_CHECKPOINT_PATH = (\"models/cnn/512_mineseg-cnn_epoch-10_val-iou-0.5313.ckpt\")\n",
    "CHIP_SIZE = 512\n",
    "TESTSET_BATCH_SIZE = 8 # 64 for L4, 8 for PC GPU\n",
    "TRAIN_CHIP_DIR = \"data/processed/chips/npy/512/train/chips/\"\n",
    "TRAIN_LABEL_DIR = \"data/processed/chips/npy/512/train/labels/\"\n",
    "VAL_CHIP_DIR = \"data/processed/chips/npy/512/val/chips/\"\n",
    "VAL_LABEL_DIR = \"data/processed/chips/npy/512/val/labels/\"\n",
    "TEST_CHIP_DIR = \"data/processed/chips/npy/512/test/chips/\"\n",
    "TEST_LABEL_DIR = \"data/processed/chips/npy/512/test/labels/\"\n",
    "\n",
    "\n",
    "DATASET = \"data/processed/mining_tiles_with_masks_and_bounding_boxes.gpkg\"\n",
    "METADATA_PATH = \"configs/cnn/cnn_segment_metadata.yaml\"\n",
    "BATCH_SIZE = 1\n",
    "if torch.cuda.is_available():\n",
    "    NUM_WORKERS = 16\n",
    "else:\n",
    "    NUM_WORKERS = 4\n",
    "PLATFORM = \"sentinel-2-l2a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdcceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_lightning = True\n",
    "if is_lightning:\n",
    "    MINESEG_CHECKPOINT_PATH = root + \"/\" + MINESEG_CHECKPOINT_PATH\n",
    "    METADATA_PATH = root + \"/\" + METADATA_PATH\n",
    "    TRAIN_CHIP_DIR = root +  \"/\" +TRAIN_CHIP_DIR\n",
    "    TRAIN_LABEL_DIR = root + \"/\" + TRAIN_LABEL_DIR\n",
    "    VAL_CHIP_DIR = root + \"/\" + VAL_CHIP_DIR\n",
    "    VAL_LABEL_DIR = root + \"/\" + VAL_LABEL_DIR\n",
    "    TEST_CHIP_DIR = root + \"/\" + TEST_CHIP_DIR\n",
    "    TEST_LABEL_DIR = root + \"/\" + TEST_LABEL_DIR\n",
    "    DATASET = root + \"/\" + DATASET\n",
    "    # print(f\"Using model at {MINESEG_CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3fc9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = MINESEG_CHECKPOINT_PATH.split(\"/\")[-1]\n",
    "print(f\"Using model {model_name}\")\n",
    "print(f\"Using chip size {CHIP_SIZE}\")\n",
    "print(f\"Using test chip dir {TEST_CHIP_DIR}\")\n",
    "print(f\"Using test label dir {TEST_LABEL_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc278db5-e241-4763-8f33-bdeb5b0f81fc",
   "metadata": {},
   "source": [
    "### Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0da577-f3e5-485a-bbc5-a3ff7367e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(checkpoint_path: str) -> MineSegmentorCNN:\n",
    "    # check if gpu is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    map_location=torch.device(device)\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
    "    model_config = checkpoint[\"hyper_parameters\"]\n",
    "    model = MineSegmentorCNN.load_from_checkpoint(checkpoint_path, **model_config)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9ba7fc-f1ca-465c-be66-15edca8e8419",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3402cf0a-cb9b-47c4-a12a-bb704912edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(\n",
    "    train_chip_dir,\n",
    "    train_label_dir,\n",
    "    val_chip_dir,\n",
    "    val_label_dir,\n",
    "    test_chip_dir,\n",
    "    test_label_dir,\n",
    "    metadata_path,\n",
    "    batch_size,\n",
    "    num_workers,\n",
    "    platform,\n",
    "    data_augmentation,\n",
    "    index=None\n",
    "):\n",
    "    dm = MineDataModule(\n",
    "        train_chip_dir=train_chip_dir,\n",
    "        train_label_dir=train_label_dir,\n",
    "        val_chip_dir=val_chip_dir,\n",
    "        val_label_dir=val_label_dir,\n",
    "        test_chip_dir=test_chip_dir,\n",
    "        test_label_dir=test_label_dir,\n",
    "        metadata_path=metadata_path,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        platform=platform,\n",
    "        data_augmentation=data_augmentation,\n",
    "    )\n",
    "    \n",
    "    dm.setup(stage=\"test\")\n",
    "    \n",
    "    if index is not None:\n",
    "        test_dl = iter(dm.test_dataloader())\n",
    "        for i in range(index + 1):\n",
    "            batch = next(test_dl)\n",
    "        metadata = dm.metadata\n",
    "        return batch, metadata\n",
    "    else:\n",
    "        test_dl = dm.test_dataloader()\n",
    "        batch = next(iter(test_dl))\n",
    "        metadata = dm.metadata\n",
    "        return batch, metadata\n",
    "    metadata = dm.metadata\n",
    "    return batch, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea94afc8-c507-41b8-a3be-dd130ff90c72",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d71514-47b0-447b-899b-5aef44c38bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prediction(model, batch):\n",
    "    with torch.no_grad():\n",
    "        image = batch[\"pixels\"]\n",
    "        outputs = model(image)\n",
    "    outputs = F.interpolate(\n",
    "        outputs, size=(CHIP_SIZE, CHIP_SIZE), mode=\"bilinear\", align_corners=False\n",
    "    )\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a64735f-70b1-4d05-acd9-2a0812545cfa",
   "metadata": {},
   "source": [
    "### Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d69561e-b7ab-4f4d-b426-2d0cccc949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(batch, outputs, metadata, index=0):\n",
    "    prob_mask = outputs.cpu().numpy()\n",
    "    pred_mask = (prob_mask > 0.5).astype(float)\n",
    "    labels = batch[\"label\"].detach().cpu().numpy()\n",
    "    pixels = batch[\"pixels\"].detach().cpu().numpy()\n",
    "\n",
    "    # normalize and clip the image ranges\n",
    "    pixels = (pixels - pixels.min()) / (pixels.max() - pixels.min())\n",
    "    pixels = np.clip(pixels, 0, 1)\n",
    "\n",
    "    images = pixels[index]\n",
    "    labels = labels[index]\n",
    "    prob_mask = prob_mask[index]\n",
    "    pred_mask = pred_mask[index].astype(float)\n",
    "\n",
    "    images = images.transpose((1,2,0))\n",
    "    prob_mask = prob_mask.transpose((1,2,0))\n",
    "    pred_mask = pred_mask.transpose((1,2,0)).astype(float)\n",
    "\n",
    "    # normalize the probablity mask\n",
    "    prob_mask = (prob_mask - prob_mask.min()) / (prob_mask.max() - prob_mask.min())\n",
    "\n",
    "    return images, labels, prob_mask, pred_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef86d23c-eca7-458a-99ef-fff4534b927e",
   "metadata": {},
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368b1925-be0f-47a5-bbb9-c642c3f04afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(images, labels, probas, preds):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(15, 6))\n",
    "\n",
    "    # Plot the image\n",
    "    axes[0].imshow(images)\n",
    "    axes[0].axis(\"off\")\n",
    "    axes[0].set_title(\"Image\", fontsize=12)\n",
    "\n",
    "    # Plot the actual segmentation\n",
    "    axes[1].imshow(labels, vmin=0, vmax=1)\n",
    "    axes[1].axis(\"off\")\n",
    "    axes[1].set_title(\"Actual\", fontsize=12)\n",
    "\n",
    "    # Plot the predicted segmentation\n",
    "    axes[2].imshow(preds, vmin=0, vmax=1)\n",
    "    axes[2].axis(\"off\")\n",
    "    axes[2].set_title(\"Pred\", fontsize=12)\n",
    "\n",
    "    # Plot the predicted segmentation\n",
    "    axes[3].imshow(probas, vmin=0, vmax=1)\n",
    "    axes[3].axis(\"off\")\n",
    "    axes[3].set_title(\"Proba\", fontsize=12)\n",
    "\n",
    "    # Plot the plot_pred_vs_true_mask\n",
    "    plot_pred_vs_true_mask(images, labels, preds.squeeze(), ax=axes[4], add_legend=False)\n",
    "    axes[4].set_title(\"Pred vs True\", fontsize=12)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9b66b-ea25-4697-83be-776abb40db9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = get_model(MINESEG_CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0eac75",
   "metadata": {},
   "source": [
    "## Plot example predictions\n",
    "\n",
    "7 Random chips, including their ground truth and predicted masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995dbbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chips = os.listdir(TEST_CHIP_DIR)\n",
    "test_chips_indices = [test_chips.index(chip) for chip in test_chips]\n",
    "test_chips_wo_miningarea = [chip for chip in test_chips if \"nominearea\" in chip]\n",
    "test_chips_w_miningarea = [chip for chip in test_chips if \"nominearea\" not in chip]\n",
    "indices_wo_miningarea = [test_chips.index(chip) for chip in test_chips_wo_miningarea]\n",
    "indices_w_miningarea = [test_chips.index(chip) for chip in test_chips_w_miningarea]\n",
    "\n",
    "print(f\"Number of test chips: {len(test_chips)}\")\n",
    "print(f\"Number of test chips without mining area: {len(test_chips_wo_miningarea)}\")\n",
    "print(f\"Number of test chips with mining area: {len(test_chips_w_miningarea)}\")\n",
    "\n",
    "# take a sample of the chips with mining area \n",
    "# random.seed(42)\n",
    "sample_indices_w_ma = random.sample(indices_w_miningarea, 4)\n",
    "print(f\"Sample indices of chips with mining area: {sample_indices_w_ma}\")\n",
    "\n",
    "sample_indices_wo_ma = random.sample(indices_wo_miningarea, 4)\n",
    "print(f\"Sample indices of chips without mining area: {sample_indices_wo_ma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac07a050-c55d-4392-9461-a16afdb65f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_predictions(sample_indices):\n",
    "    for index in sample_indices:\n",
    "\n",
    "        # Get data\n",
    "        batch, metadata = get_data(\n",
    "            TRAIN_CHIP_DIR,\n",
    "            TRAIN_LABEL_DIR,\n",
    "            VAL_CHIP_DIR,\n",
    "            VAL_LABEL_DIR,\n",
    "            TEST_CHIP_DIR,\n",
    "            TEST_LABEL_DIR,\n",
    "            METADATA_PATH,\n",
    "            BATCH_SIZE,\n",
    "            NUM_WORKERS,\n",
    "            PLATFORM,\n",
    "            data_augmentation=False,\n",
    "            index=index\n",
    "        )\n",
    "\n",
    "        # Move batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "        # Run prediction\n",
    "        outputs = run_prediction(model, batch)\n",
    "\n",
    "        # Post-process the results\n",
    "        images, labels, probas, preds = post_process(batch, outputs, metadata)\n",
    "\n",
    "        # Plot the predictions\n",
    "        plot_predictions(images, labels, probas, preds)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c6281f",
   "metadata": {},
   "source": [
    "Predictions on chips with mining area: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a68bfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_predictions(sample_indices_w_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37790c07",
   "metadata": {},
   "source": [
    "Predictions on chips without mining area:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae749c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_predictions(sample_indices_wo_ma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5cb06",
   "metadata": {},
   "source": [
    "## Metric calculation on test dataset\n",
    "\n",
    "To calculate the metric on the test dataset, we need to predict the masks for all the chips in the test dataset and then calculate an aggregate metric.\n",
    "\n",
    "\n",
    "By default, the test set includes only chips with mining area. However, this will cover differently large areas of the actual tiles. Therefore, we will also calculate the metric on the full test set, which includes all chips from the test tiles. This makes models for different chip sizes comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf4de7e",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45d98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_to_temp_directory(tile_idx, test_chip_dir, test_label_dir):\n",
    "    # List all files in the test chip directory\n",
    "    test_chips = os.listdir(test_chip_dir)\n",
    "    test_masks = os.listdir(test_label_dir)\n",
    "\n",
    "    # Create a new directory at the same level as TEST_CHIP_DIR with the name temp\n",
    "    greatgrandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(TEST_CHIP_DIR)))\n",
    "    temp_dir = os.path.join(greatgrandparent_dir, \"temp\")\n",
    "    Path(os.path.join(temp_dir, \"chips\")).mkdir(parents=True, exist_ok=True)\n",
    "    Path(os.path.join(temp_dir, \"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    temp_chip_dir = os.path.join(temp_dir, \"chips/\")\n",
    "    temp_label_dir = os.path.join(temp_dir, \"labels/\")\n",
    "\n",
    "    # Remove any existing files in the directories\n",
    "    for chip in os.listdir(temp_chip_dir):\n",
    "        os.remove(os.path.join(temp_chip_dir, chip))\n",
    "    for mask in os.listdir(temp_label_dir):\n",
    "        os.remove(os.path.join(temp_label_dir, mask))\n",
    "\n",
    "    # Get unique tile ids\n",
    "    tile_ids_chips = [chip.split(\"_\")[0] for chip in test_chips]\n",
    "    tile_ids_masks = [mask.split(\"_\")[0] for mask in test_masks]\n",
    "    unique_tile_ids_chips = set(tile_ids_chips)\n",
    "    unique_tile_ids_masks = set(tile_ids_masks)\n",
    "\n",
    "    assert unique_tile_ids_chips == unique_tile_ids_masks\n",
    "    unique_tile_ids = sorted(list(unique_tile_ids_chips))\n",
    "\n",
    "    # Get the indices where tile_ids is equal to the specified tile_idx\n",
    "    tile_id_indices_chips = [i for i, x in enumerate(tile_ids_chips) if x == unique_tile_ids[tile_idx]]\n",
    "    tile_id_indices_masks = [i for i, x in enumerate(tile_ids_masks) if x == unique_tile_ids[tile_idx]]\n",
    "\n",
    "    # Move files at the respective indices to the temp directory\n",
    "    for index in tile_id_indices_chips:\n",
    "        # Move chip file\n",
    "        shutil.copy(os.path.join(TEST_CHIP_DIR, test_chips[index]), os.path.join(temp_chip_dir, test_chips[index]))\n",
    "\n",
    "    for index in tile_id_indices_masks:\n",
    "        # Move mask file\n",
    "        shutil.copy(os.path.join(TEST_LABEL_DIR, test_masks[index]), os.path.join(temp_label_dir, test_masks[index]))\n",
    "\n",
    "    # print(f\"Copied chips to {temp_chip_dir}\")\n",
    "    # print(f\"Copied labels to {temp_label_dir}\")\n",
    "    # print(f\"Number of chips: {len(os.listdir(temp_chip_dir))}\")\n",
    "    # print(\"Filename: \", test_chips[index])\n",
    "    return temp_chip_dir, temp_label_dir, test_chips[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f359c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(model, test_chip_dir, test_label_dir, testset_batch_size, calculate_per_tile=False):\n",
    "    \"\"\"\n",
    "    Calculates various metrics for evaluating the performance of a model on a test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The model to evaluate.\n",
    "        test_chip_dir (str): The directory containing the test dataset chips.\n",
    "        test_label_dir (str): The directory containing the test dataset labels.\n",
    "        testset_batch_size (int): The batch size for processing the test dataset.\n",
    "        calculate_per_tile (bool): If True, the metrics will be calculated for each tile in the test dataset.\n",
    "        \n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the calculated metrics for each image in the test dataset.\n",
    "            The DataFrame has the following columns: 'image', 'file_name', 'iou', 'f1', 'accuracy', 'recall', 'precision'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a DataFrame to store the results\n",
    "    results = pd.DataFrame(columns=['image', \"file_name\", 'iou', 'f1', 'accuracy', 'recall', 'precision', 'iou_custom', 'f1_custom', 'accuracy_custom', 'recall_custom', 'precision_custom'])\n",
    "\n",
    "    # Get the list of test chips\n",
    "    test_chips = os.listdir(test_chip_dir)\n",
    "\n",
    "    # Calculate number of batches\n",
    "    num_batches = int(np.ceil(len(test_chips) / testset_batch_size))\n",
    "\n",
    "    # Move model to GPU\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.to(\"cuda\")\n",
    "        \n",
    "    for i in tqdm(range(num_batches)):\n",
    "        \n",
    "        if calculate_per_tile:\n",
    "            chip_dir, label_dir, file_name = copy_files_to_temp_directory(i, test_chip_dir, test_label_dir)\n",
    "            index=0\n",
    "            file_names = [file_name]\n",
    "        else:\n",
    "            # Get the file names for the current batch\n",
    "            file_names = test_chips[i * testset_batch_size : (i + 1) * testset_batch_size]\n",
    "            index=i\n",
    "            chip_dir = test_chip_dir\n",
    "            label_dir = test_label_dir\n",
    "\n",
    "        # Get data\n",
    "        batch, metadata = get_data(\n",
    "            TRAIN_CHIP_DIR,\n",
    "            TRAIN_LABEL_DIR,\n",
    "            VAL_CHIP_DIR,\n",
    "            VAL_LABEL_DIR,\n",
    "            chip_dir,\n",
    "            label_dir,\n",
    "            METADATA_PATH,\n",
    "            testset_batch_size,\n",
    "            NUM_WORKERS,\n",
    "            PLATFORM,\n",
    "            data_augmentation=False,\n",
    "            index=index\n",
    "        )\n",
    "        \n",
    "        # Move batch to GPU\n",
    "        if torch.cuda.is_available():\n",
    "            batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "        # Run prediction\n",
    "        outputs = run_prediction(model, batch)\n",
    "\n",
    "        labels = batch[\"label\"].detach().cpu().numpy()\n",
    "        preds = outputs.detach().cpu().numpy()\n",
    "        preds = (preds > 0.5).astype(float)\n",
    "\n",
    "        # # combine all chips to one tile\n",
    "        if calculate_per_tile:\n",
    "            labels = labels.flatten()\n",
    "            preds = preds.flatten()\n",
    "            \n",
    "        # Calculate the metrics for each image in the batch\n",
    "        for j in range(len(file_names)):\n",
    "\n",
    "            if calculate_per_tile:\n",
    "                true_mask = labels\n",
    "                pred_mask = preds\n",
    "            else:\n",
    "                true_mask = labels[j].flatten()\n",
    "                pred_mask = preds[j].flatten()\n",
    "\n",
    "            iou = jaccard_score(true_mask, pred_mask)\n",
    "            f1 = f1_score(true_mask, pred_mask)\n",
    "            accuracy = accuracy_score(true_mask, pred_mask)\n",
    "            recall = recall_score(true_mask, pred_mask)\n",
    "            precision = precision_score(true_mask, pred_mask)\n",
    "\n",
    "            ### custom metrics\n",
    "            # Calculate different areas\n",
    "            true_positive = np.logical_and(true_mask == 1, pred_mask == 1)\n",
    "            true_negative = np.logical_and(true_mask == 0, pred_mask == 0)\n",
    "            false_positive = np.logical_and(true_mask == 0, pred_mask == 1)\n",
    "            false_negative = np.logical_and(true_mask == 1, pred_mask == 0)\n",
    "\n",
    "            # Calculate metrics\n",
    "            if np.all(true_mask == 0) and np.all(pred_mask == 0):\n",
    "                iou_custom = 1.0\n",
    "                precision_custom = 1.0\n",
    "                accuracy_custom = 1.0\n",
    "                recall_custom = 1.0\n",
    "                f1_custom = 1.0\n",
    "            else:\n",
    "                intersection = np.logical_and(true_mask, pred_mask)\n",
    "                union = np.logical_or(true_mask, pred_mask)\n",
    "                iou_custom = np.sum(intersection) / np.sum(union)\n",
    "                accuracy_custom = (np.sum(true_positive) + np.sum(true_negative)) / (np.sum(true_positive) + np.sum(true_negative) + np.sum(false_positive) + np.sum(false_negative))\n",
    "                precision_custom = np.sum(true_positive) / (np.sum(true_positive) + np.sum(false_positive))\n",
    "                recall_custom = np.sum(true_positive) / (np.sum(true_positive) + np.sum(false_negative))\n",
    "                f1_custom = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "            # Add the results to the DataFrame\n",
    "            results = pd.concat(\n",
    "                [results, \n",
    "                pd.DataFrame(\n",
    "                    {\n",
    "                        'image': [i * testset_batch_size + j], \n",
    "                        'file_name': [file_names[j]],\n",
    "                        'iou': [iou], \n",
    "                        'f1': [f1], \n",
    "                        'accuracy': [accuracy], \n",
    "                        'recall': [recall], \n",
    "                        'precision': [precision],\n",
    "                        'iou_custom': [iou_custom],\n",
    "                        'f1_custom': [f1_custom],\n",
    "                        'accuracy_custom': [accuracy_custom],\n",
    "                        'recall_custom': [recall_custom],\n",
    "                        'precision_custom': [precision_custom],\n",
    "                    })])\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aa217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_grouped_metrics(results_grouped):\n",
    "    # Metrics per mine type\n",
    "    print(\"Metrics per mine type (minetype1):\")\n",
    "    print(results_grouped.groupby(\"minetype1\")[[\"iou\", \"f1\", \"accuracy\", \"recall\", \"precision\"]].mean())\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    # Metrics per mine type\n",
    "    print(\"Metrics per mine type (minetype2):\")\n",
    "    print(results_grouped.groupby(\"minetype2\")[[\"iou\", \"f1\", \"accuracy\", \"recall\", \"precision\"]].mean())\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    # Metrics per dataset\n",
    "    print(\"Metrics per dataset (preferred_dataset):\")\n",
    "    print(results_grouped.groupby(\"preferred_dataset\")[[\"iou\", \"f1\", \"accuracy\", \"recall\", \"precision\"]].mean())\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    # print a histogram of the iou scores\n",
    "    print(\"Histogram of IoU scores:\")\n",
    "    results_grouped[\"iou\"].hist(bins=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921efa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_iou_on_test_tiles(results_grouped):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 25))\n",
    "\n",
    "    # Assuming test_tiles is your GeoDataFrame\n",
    "    results_grouped = results_grouped.set_crs(epsg=4326)\n",
    "\n",
    "    # Convert the GeoDataFrame to Web Mercator projection (EPSG:3857)\n",
    "    results_grouped = results_grouped.to_crs(epsg=3857)\n",
    "\n",
    "    # Buffer the polygons to increase their size\n",
    "    results_grouped['geometry'] = results_grouped.geometry.buffer(200000)\n",
    "\n",
    "    # Plot the GeoDataFrame\n",
    "    results_grouped.plot(column='iou', cmap='RdYlGn', linewidth=0.8, ax=ax, edgecolor='0.8', alpha=1)\n",
    "\n",
    "    # Add a basemap\n",
    "    ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "    ax.axis('off')\n",
    "    ax.set_title('IoU on test tiles', fontdict={'fontsize': '25', 'fontweight' : '3'})\n",
    "\n",
    "    # Create a colorbar as a legend\n",
    "    sm = plt.cm.ScalarMappable(cmap='RdYlGn', norm=plt.Normalize(vmin=min(results_grouped['iou']), vmax=max(results_grouped['iou'])))\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=ax, orientation='horizontal', fraction=0.03, pad=0.04)\n",
    "    cbar.ax.tick_params(labelsize=8)  # set the size of the colorbar labels\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9919bed0",
   "metadata": {},
   "source": [
    "Create the new directory for the report and the data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278e539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = root + f\"/reports/cnn/{''.join(model_name.split('.')[:-1])}\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving results to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdbe498",
   "metadata": {},
   "source": [
    "### On the test set chips (mining area chips only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d1e9ad",
   "metadata": {},
   "source": [
    "Copy Chips and Labels with mining area to a separate directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the test chip directory\n",
    "test_chips = os.listdir(TEST_CHIP_DIR)\n",
    "test_masks = os.listdir(TEST_LABEL_DIR)\n",
    "\n",
    "# Create a new directory at the same level as TEST_CHIP_DIR with the name test_minearea_only\n",
    "greatgrandparent_dir = os.path.dirname(os.path.dirname(os.path.dirname(TEST_CHIP_DIR)))\n",
    "test_minearea_only_dir = os.path.join(greatgrandparent_dir, \"test_minearea_only\")\n",
    "Path(os.path.join(test_minearea_only_dir, \"chips\")).mkdir(parents=True, exist_ok=True)\n",
    "Path(os.path.join(test_minearea_only_dir, \"labels\")).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "minearea_chip_dir = os.path.join(test_minearea_only_dir, \"chips/\")\n",
    "minearea_label_dir = os.path.join(test_minearea_only_dir, \"labels/\")\n",
    "\n",
    "# Move files containing \"nominearea\" to the respective directories\n",
    "for chip in tqdm(test_chips):\n",
    "    if \"nominearea\" not in chip:\n",
    "        # Move chip file\n",
    "        shutil.copy(os.path.join(TEST_CHIP_DIR, chip), os.path.join(minearea_chip_dir, chip))\n",
    "\n",
    "# Move files containing \"nominearea\" to the respective directories\n",
    "for mask in tqdm(test_masks):\n",
    "    if \"nominearea\" not in mask:\n",
    "        # Move mask file\n",
    "        shutil.copy(os.path.join(TEST_LABEL_DIR, mask), os.path.join(minearea_label_dir, mask))\n",
    "\n",
    "print(f\"Copied chips with mining area to {minearea_chip_dir}\")\n",
    "print(f\"Copied labels with mining area to {minearea_label_dir}\")\n",
    "print(f\"Number of chips with mining area: {len(os.listdir(minearea_chip_dir))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7333edc9",
   "metadata": {},
   "source": [
    "Calculate the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca50a336",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = calculate_metrics(model, minearea_chip_dir, minearea_label_dir, TESTSET_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6caca4",
   "metadata": {},
   "source": [
    "Aggregate metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9856fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the processed dataset\n",
    "tiles = gpd.read_file(DATASET, layer=\"tiles\")\n",
    "test_tiles = tiles[tiles[\"split\"] == \"test\"]\n",
    "\n",
    "# extract the tile_id from the file_name\n",
    "results[\"tile_id\"] = results[\"file_name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "# convert tile_id to int\n",
    "results['tile_id'] = results['tile_id'].astype(int)\n",
    "\n",
    "# Merge the results with the test_tiles\n",
    "results_merged = results.merge(test_tiles[[\"tile_id\", \"preferred_dataset\", \"minetype1\", \"minetype2\"]], on='tile_id')\n",
    "\n",
    "# # save as csv in the reports folder\n",
    "output_path = output_dir + \"/testset_metrics_per_chip.csv\"\n",
    "test_tiles.to_csv(output_path, index=False)\n",
    "print(f\"Saved aggregated metrics to {output_path}\")\n",
    "\n",
    "# calculate overall metrics\n",
    "iou = results_merged[\"iou\"].mean()\n",
    "f1 = results_merged[\"f1\"].mean()\n",
    "accuracy = results_merged[\"accuracy\"].mean()\n",
    "recall = results_merged[\"recall\"].mean()\n",
    "precision = results_merged[\"precision\"].mean()\n",
    "custom_iou = results[\"iou_custom\"].mean()\n",
    "custom_f1 = results[\"f1_custom\"].mean()\n",
    "custom_accuracy = results[\"accuracy_custom\"].mean()\n",
    "custom_recall = results[\"recall_custom\"].mean()\n",
    "custom_precision = results[\"precision_custom\"].mean()\n",
    "\n",
    "# print the results\n",
    "print(f\"Mean IoU: {iou}\")\n",
    "print(f\"Mean F1: {f1}\")\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "print(f\"Mean Recall: {recall}\")\n",
    "print(f\"Mean Precision: {precision}\")\n",
    "print(f\"Mean Custom IoU: {custom_iou}\")\n",
    "print(f\"Mean Custom F1: {custom_f1}\")\n",
    "print(f\"Mean Custom Accuracy: {custom_accuracy}\")\n",
    "print(f\"Mean Custom Recall: {custom_recall}\")\n",
    "print(f\"Mean Custom Precision: {custom_precision}\")\n",
    "results_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e8689",
   "metadata": {},
   "source": [
    "Grouped metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a2e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grouped_metrics(results_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5399ae3c",
   "metadata": {},
   "source": [
    "### On the whole test set tile (including chips without mining area)\n",
    "This allows us to compare models with different chip sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3878e",
   "metadata": {},
   "source": [
    "Calculate the metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf624ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the batch size, so that one batch includes all chips for a tile\n",
    "batch_size_tile = int((2048/CHIP_SIZE)**2)\n",
    "print(f\"Batch size: {batch_size_tile}\")\n",
    "\n",
    "results = calculate_metrics(model, TEST_CHIP_DIR, TEST_LABEL_DIR, batch_size_tile, calculate_per_tile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c43f12",
   "metadata": {},
   "source": [
    "Aggregate metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634a0262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the processed dataset\n",
    "tiles = gpd.read_file(DATASET, layer=\"tiles\")\n",
    "test_tiles = tiles[tiles[\"split\"] == \"test\"]\n",
    "\n",
    "# extract the tile_id from the file_name\n",
    "results[\"tile_id\"] = results[\"file_name\"].apply(lambda x: x.split(\"_\")[0])\n",
    "\n",
    "# convert tile_id to int\n",
    "results['tile_id'] = results['tile_id'].astype(int)\n",
    "\n",
    "# Merge the results with the test_tiles\n",
    "results_merged = results.merge(test_tiles[[\"tile_id\", \"preferred_dataset\", \"minetype1\", \"minetype2\", \"geometry\"]], on='tile_id')\n",
    "\n",
    "# # save as csv in the reports folder\n",
    "output_path = output_dir + \"/testset_metrics_per_tile.csv\"\n",
    "test_tiles.to_csv(output_path, index=False)\n",
    "print(f\"Saved aggregated metrics to {output_path}\")\n",
    "\n",
    "# calculate overall metrics\n",
    "iou = results_merged[\"iou\"].mean()\n",
    "f1 = results_merged[\"f1\"].mean()\n",
    "accuracy = results_merged[\"accuracy\"].mean()\n",
    "recall = results_merged[\"recall\"].mean()\n",
    "precision = results_merged[\"precision\"].mean()\n",
    "custom_iou = results[\"iou_custom\"].mean()\n",
    "custom_f1 = results[\"f1_custom\"].mean()\n",
    "custom_accuracy = results[\"accuracy_custom\"].mean()\n",
    "custom_recall = results[\"recall_custom\"].mean()\n",
    "custom_precision = results[\"precision_custom\"].mean()\n",
    "\n",
    "# print the results\n",
    "print(f\"Mean IoU: {iou}\")\n",
    "print(f\"Mean F1: {f1}\")\n",
    "print(f\"Mean Accuracy: {accuracy}\")\n",
    "print(f\"Mean Recall: {recall}\")\n",
    "print(f\"Mean Precision: {precision}\")\n",
    "print(f\"Mean Custom IoU: {custom_iou}\")\n",
    "print(f\"Mean Custom F1: {custom_f1}\")\n",
    "print(f\"Mean Custom Accuracy: {custom_accuracy}\")\n",
    "print(f\"Mean Custom Recall: {custom_recall}\")\n",
    "print(f\"Mean Custom Precision: {custom_precision}\")\n",
    "results_merged[['tile_id', 'preferred_dataset', 'minetype1', 'minetype2', 'iou', 'f1', 'accuracy', 'recall', 'precision', 'iou_custom', 'f1_custom', 'accuracy_custom', 'recall_custom', 'precision_custom']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eba4ff",
   "metadata": {},
   "source": [
    "Grouped metrics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc152aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_grouped_metrics(results_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a3458",
   "metadata": {},
   "source": [
    "IoU per tile on the map: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29e807",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_merged_gdf = gpd.GeoDataFrame(results_merged, geometry=results_merged[\"geometry\"])\n",
    "\n",
    "plot_iou_on_test_tiles(results_merged_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8606b4f4",
   "metadata": {},
   "source": [
    "# Plot individual tiles and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a97ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_dir, label_dir, file_name = copy_files_to_temp_directory(6, TEST_CHIP_DIR, TEST_LABEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfc89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data\n",
    "batch, metadata = get_data(\n",
    "    TRAIN_CHIP_DIR,\n",
    "    TRAIN_LABEL_DIR,\n",
    "    VAL_CHIP_DIR,\n",
    "    VAL_LABEL_DIR,\n",
    "    chip_dir,\n",
    "    label_dir,\n",
    "    METADATA_PATH,\n",
    "    16,\n",
    "    NUM_WORKERS,\n",
    "    PLATFORM,\n",
    "    data_augmentation=False,\n",
    "    index=0\n",
    ")\n",
    "\n",
    "# Move batch to GPU\n",
    "if torch.cuda.is_available():\n",
    "    batch = {k: v.to(\"cuda\") for k, v in batch.items()}\n",
    "\n",
    "# Run prediction\n",
    "outputs = run_prediction(model, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb298d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list all files in the temp directory\n",
    "temp_chips = os.listdir(chip_dir)\n",
    "\n",
    "# extract the chip number\n",
    "chip_num = [int(chip.split(\"_\")[9].split(\".\")[0]) for chip in temp_chips]\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "probas = []\n",
    "preds = []\n",
    "\n",
    "# Iterate over the indices in the batch\n",
    "for i in range(16):\n",
    "    index = chip_num.index(i)\n",
    "    # print(f\"Processing chip {temp_chips[index]}\")\n",
    "    # Post-process the results\n",
    "    image, label, proba, pred = post_process(batch, outputs, metadata, index=index)\n",
    "    images.append(image)\n",
    "    labels.append(label)\n",
    "    probas.append(proba.squeeze())\n",
    "    preds.append(pred.squeeze())\n",
    "\n",
    "# Combine the 16 images into a single 2048x2048 image\n",
    "def put_np_together(images, channels=3):\n",
    "    if channels == 1:\n",
    "        big_image = np.zeros((2048, 2048))\n",
    "    elif channels == 3:\n",
    "        big_image = np.zeros((2048, 2048, 3))\n",
    "\n",
    "    # Define the chip size and the number of chips in each dimension\n",
    "    chip_size = 512\n",
    "    n_chips_x = 4\n",
    "    n_chips_y = 4\n",
    "\n",
    "    # Iterate over the 16 images and place them in the correct position\n",
    "    chip_number = 0\n",
    "    for i in range(n_chips_x):  # Iterate over columns\n",
    "        for j in range(n_chips_y):  # Iterate over rows\n",
    "            x1, y1 = j * chip_size, i * chip_size\n",
    "            x2, y2 = x1 + chip_size, y1 + chip_size\n",
    "            if channels == 1:\n",
    "                big_image[x1:x2, y1:y2] = images[chip_number]\n",
    "            else:\n",
    "                big_image[x1:x2, y1:y2, :] = images[chip_number]\n",
    "            chip_number += 1\n",
    "\n",
    "    return big_image\n",
    "\n",
    "big_image = put_np_together(images, channels=3)\n",
    "big_label = put_np_together(labels, channels=1)\n",
    "big_proba = put_np_together(probas, channels=1)\n",
    "big_pred = put_np_together(preds, channels=1)\n",
    "\n",
    "plot_predictions(big_image, big_label, big_proba, big_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f82490",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pred_vs_true_mask(big_image, big_label, big_pred, add_legend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7da98",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_path = root + \"/notebooks/cnn/cnn_inference.ipynb\"\n",
    "output_dir_report = output_dir + \"/\"\n",
    "print(f\"Exporting notebook to {output_dir_report}\")\n",
    "\n",
    "# export the notebook to reports\n",
    "!jupyter nbconvert --to html $nb_path --output-dir=$output_dir_report --output=\"CNN_testset_evaluation.html\" --no-input --no-prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60063040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping mining areas with DINO & SAM\n",
    "\n",
    "This notebook aims to detect and map mining areas using the Grounding DINO model for Object Detection, the Segment Anything Model (SAM) for segmentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "\n",
    "from src.models.samgeo.model import MineSamGeo\n",
    "from src.utils import geotiff_to_PIL\n",
    "from src.visualization.visualization_funcs import plot_pred_vs_true_mask, plot_predictions\n",
    "\n",
    "# check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# set working directory to root \n",
    "import os\n",
    "os.chdir(\"../../\")\n",
    "root = os.getcwd()\n",
    "# root = root + \"/workspaces/mine-segmentation\" # for lightning studios\n",
    "print(f\"Root directory: {root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chip_folder = root + \"/data/processed/chips/train/chips\"\n",
    "# chip_mask_folder = root + \"/data/processed/chips/train/labels\"\n",
    "# output_folder = root + \"/data/output\"\n",
    "\n",
    "TEST_CHIP_DIR = \"data/processed/chips/tif/2048/test/chips/\"\n",
    "TEST_LABEL_DIR = \"data/processed/chips/tif/2048/test/labels/\"\n",
    "VAL_CHIP_DIR = \"data/processed/chips/tif/2048/val/chips/\"\n",
    "VAL_LABEL_DIR = \"data/processed/chips/tif/2048/val/labels/\"\n",
    "OUTPUT_DIR = \"reports/samgeo/\"\n",
    "\n",
    "TEST_CHIP_DIR = root + \"/\" + TEST_CHIP_DIR\n",
    "TEST_LABEL_DIR = root + \"/\" + TEST_LABEL_DIR\n",
    "VAL_CHIP_DIR = root + \"/\" + VAL_CHIP_DIR\n",
    "VAL_LABEL_DIR = root + \"/\" + VAL_LABEL_DIR\n",
    "OUTPUT_DIR = root + \"/\" + OUTPUT_DIR\n",
    "print(VAL_CHIP_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the SamGeo Model with text prompts on Mining areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for development\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = MineSamGeo(\n",
    "    model_type=\"vit_b\",\n",
    "    chips_dir=TEST_CHIP_DIR,\n",
    "    mask_dir=TEST_LABEL_DIR,\n",
    "    output_dir=OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "msg.num_chips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_path = msg.get_chip_path(0)\n",
    "\n",
    "msg.predict(\n",
    "    chip_path=chip_path,\n",
    "    text_prompt=\"extractive mine\", \n",
    "    box_threshold=0.15,\n",
    "    text_threshold=0.01\n",
    ")\n",
    "\n",
    "print(msg.model.logits)\n",
    "print(msg.model.boxes)\n",
    "print(msg.model.phrases)\n",
    "print(msg.calculate_metrics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image\n",
    "chip = geotiff_to_PIL(chip_path)\n",
    "chip = np.array(chip)\n",
    "\n",
    "# get mask\n",
    "mask_path = msg.get_mask_path(chip_path)\n",
    "with rasterio.open(mask_path) as src:\n",
    "    mask = src.read(1)\n",
    "\n",
    "# get prediction\n",
    "pred = msg.model.prediction\n",
    "\n",
    "plot_predictions(chip, mask, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show bounding boxes\n",
    "fig = msg.show_anns(\n",
    "    cmap=\"Blues\",\n",
    "    box_color=\"red\",\n",
    "    title=\"Bounding Boxes\",\n",
    "    blend=True,\n",
    "    add_boxes=True,\n",
    "    add_masks=False,\n",
    "    alpha=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only show mask\n",
    "fig = msg.show_anns(\n",
    "    cmap=\"Blues\",\n",
    "    title=\"Prediction Mask\",\n",
    "    blend=True,\n",
    "    add_boxes=False,\n",
    "    add_masks=True,\n",
    "    alpha=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only ground truth mask\n",
    "fig = msg.show_true_mask(\n",
    "    cmap=\"Blues\",\n",
    "    title=\"Prediction Mask\",\n",
    "    blend=True,\n",
    "    add_boxes=False,\n",
    "    add_masks=True,\n",
    "    alpha=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show ground truth mask and prediction mask\n",
    "fig = msg.show_pred_vs_true_mask(\n",
    "    blend=True,\n",
    "    add_boxes=False,\n",
    "    add_masks=True,\n",
    "    alpha=0.4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show ground truth mask and prediction mask for a random sample\n",
    "import random\n",
    "for i in random.sample(range(msg.num_chips), 7):\n",
    "    chip_path = msg.get_chip_path(i)\n",
    "\n",
    "    msg.predict(\n",
    "        chip_path=chip_path,\n",
    "        text_prompt=\"extractive site\", \n",
    "        box_threshold=0.08, \n",
    "        text_threshold=0.01,\n",
    "        box_size_threshold=0.4\n",
    "    )\n",
    "\n",
    "    # print(msg.calculate_metrics())\n",
    "\n",
    "    # Only show bounding boxes\n",
    "    fig = msg.show_anns(\n",
    "        cmap=\"Blues\",\n",
    "        box_color=\"red\",\n",
    "        title=\"Bounding Boxes\",\n",
    "        blend=True,\n",
    "        add_boxes=True,\n",
    "        add_masks=False,\n",
    "        alpha=0.1\n",
    "    )\n",
    "    \n",
    "    fig = msg.show_pred_vs_true_mask(\n",
    "        title=\"Prediction Mask\",\n",
    "        blend=True,\n",
    "        add_boxes=False,\n",
    "        add_masks=True,\n",
    "        alpha=0.4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best Hyperparameters\n",
    "\n",
    "### Prompts\n",
    "`prompt`: The prompt is an integral part of the object detection for grounding DINO. if multiple objects should be detected, prompts can be separated with a `.`, like so: `mine . city .`. \n",
    "\n",
    "`negative_prompt`: **TODO**. It is often the case that Dino detects cities or other objects that are not mines as mine. Maybe it is possible to use this to to detect negative classes (cities, forest, industrial area, deforestation), and where these are detected, the bounding boxes for any positive classes (mines) are removed. \n",
    "\n",
    "\n",
    "### Thresholds\n",
    "Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.\n",
    "\n",
    "`box_threshold`: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.\n",
    "\n",
    "`text_threshold`: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.\n",
    "\n",
    "`box_size_threshold`: This is a custom threshold used to discard bounding boxes that are too large. It represents the maximum size of the bounding box as a fraction of the image. For example, 0.5 discards all bounding boxes covering more than half the image. \n",
    "\n",
    "Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "msg = MineSamGeo(\n",
    "    model_type=\"vit_b\",\n",
    "    chips_dir=VAL_CHIP_DIR, # VAL_CHIP_DIR or TEST_CHIP_DIR\n",
    "    mask_dir=VAL_LABEL_DIR, # VAL_LABEL_DIR or TEST_LABEL_DIR\n",
    "    output_dir=OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "# Define the range of values for the hyperparameters\n",
    "text_prompts = [\"mine\", \"extractive site\", \"mine site\", \"extractive mine\"]\n",
    "box_thresholds = [.06, .08, .1]\n",
    "text_thresholds = [0.01]\n",
    "box_size_thresholds = [0.3, 0.4, 0.5]\n",
    "\n",
    "# Take best hyperparameters and run inference on test set\n",
    "# text_prompts = [\"extractive site\"]\n",
    "# box_thresholds = [.08]\n",
    "# text_thresholds = [0.01]\n",
    "# box_size_thresholds = [0.4]\n",
    "\n",
    "# # Define the number of tiles to sample\n",
    "# n_chips = 116\n",
    "# sample_indices = random.sample(range(msg.num_chips), n_chips)\n",
    "# OR \n",
    "n_chips = msg.num_chips\n",
    "sample_indices = range(msg.num_chips)\n",
    "\n",
    "# Initialize variables to store the best hyperparameters and corresponding metrics\n",
    "best_metrics = None\n",
    "best_box_threshold = None\n",
    "best_text_threshold = None\n",
    "best_box_size_threshold = None\n",
    "\n",
    "# Create an empty dataframe to store the metrics\n",
    "metrics_df = pd.DataFrame(columns=['prompt', 'box_threshold', 'text_threshold', 'box_size_threshold', 'iou', 'f1_score', 'accuracy', 'precision', 'recall'])\n",
    "\n",
    "# Perform grid search\n",
    "total_iterations = len(text_prompts) * len(box_thresholds) * len(text_thresholds) * len(box_size_thresholds) * n_chips\n",
    "progress_bar = tqdm(total=total_iterations, desc=\"Grid Search Progress\")\n",
    "\n",
    "for prompt in text_prompts:\n",
    "    for box_threshold in box_thresholds:\n",
    "        for text_threshold in text_thresholds:\n",
    "            for box_size_threshold in box_size_thresholds:\n",
    "                # Reset the metrics for each combination of hyperparameters\n",
    "                metrics = {'iou': 0, 'f1_score': 0, 'accuracy': 0, 'precision': 0, 'recall': 0}\n",
    "\n",
    "                # Perform predictions with the current hyperparameters\n",
    "                for i in sample_indices:\n",
    "                    chip_path = msg.get_chip_path(i)\n",
    "\n",
    "                    msg.predict(\n",
    "                        chip_path=chip_path,\n",
    "                        text_prompt=prompt, \n",
    "                        box_threshold=box_threshold, \n",
    "                        text_threshold=text_threshold,\n",
    "                        box_size_threshold=box_size_threshold\n",
    "                    )\n",
    "\n",
    "                    metrics_i = msg.calculate_metrics()\n",
    "\n",
    "                    # Accumulate the metrics for each chip\n",
    "                    metrics['iou'] += metrics_i['iou']\n",
    "                    metrics['f1_score'] += metrics_i['f1_score']\n",
    "                    metrics['accuracy'] += metrics_i['accuracy']\n",
    "                    metrics['precision'] += metrics_i['precision']\n",
    "                    metrics['recall'] += metrics_i['recall']\n",
    "\n",
    "                    # Update the progress bar\n",
    "                    progress_bar.update(1)\n",
    "\n",
    "                # Average the metrics over all chips\n",
    "                metrics['iou'] /= n_chips\n",
    "                metrics['f1_score'] /= n_chips\n",
    "                metrics['accuracy'] /= n_chips\n",
    "                metrics['precision'] /= n_chips\n",
    "                metrics['recall'] /= n_chips\n",
    "\n",
    "                # Add metrics to the dataframe\n",
    "                metrics_df = pd.concat(\n",
    "                    [metrics_df, pd.DataFrame({\n",
    "                        'prompt': [prompt],\n",
    "                        'box_threshold': [box_threshold],\n",
    "                        'text_threshold': [text_threshold],\n",
    "                        'box_size_threshold': [box_size_threshold],\n",
    "                        'iou': [metrics['iou']],\n",
    "                        'f1_score': [metrics['f1_score']],\n",
    "                        'accuracy': [metrics['accuracy']],\n",
    "                        'precision': [metrics['precision']],\n",
    "                        'recall': [metrics['recall']]\n",
    "                        })], ignore_index=True)\n",
    "\n",
    "# Close the progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "# Find the best performing prompt and corresponding metrics\n",
    "best_metrics = metrics_df.loc[metrics_df['f1_score'].idxmax()]\n",
    "best_prompt = best_metrics['prompt']\n",
    "best_box_threshold = best_metrics['box_threshold']\n",
    "best_text_threshold = best_metrics['text_threshold']\n",
    "best_box_size_threshold = best_metrics['box_size_threshold']\n",
    "print(\"Best prompt:\", best_prompt)\n",
    "print(\"Best box_threshold:\", best_box_threshold)\n",
    "print(\"Best text_threshold:\", best_text_threshold)\n",
    "print(\"Best box_size_threshold:\", best_box_size_threshold)\n",
    "print(\"Best metrics:\", best_metrics)\n",
    "\n",
    "# Save the metrics dataframe to a CSV file\n",
    "time = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "metrics_df.to_csv(root + f\"/reports/samgeo_gridsearch_{time}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the output csv\n",
    "metrics_df = pd.read_csv(root + \"/reports/samgeo_gridsearch_20241003_093349.csv\")\n",
    "metrics_df = metrics_df.sort_values(by='iou', ascending=False)\n",
    "top_5_df = metrics_df.head(5)\n",
    "\n",
    "# reset index\n",
    "top_5_df = top_5_df.reset_index(drop=True)\n",
    "\n",
    "hypers_to_plot = ['prompt', 'box_threshold', 'text_threshold', 'box_size_threshold']\n",
    "\n",
    "top_5_df['hyperparams_str'] = top_5_df[hypers_to_plot].apply(lambda x: '\\n'.join(x.astype(str)), axis=1)\n",
    "\n",
    "# add explanations to the first row\n",
    "first_row = top_5_df.loc[0,\"hyperparams_str\"]\n",
    "\n",
    "# split the string by newline\n",
    "first_row = first_row.split(\"\\n\")\n",
    "\n",
    "# insert explanations\n",
    "first_row[0] = f\"prompt: {first_row[0]}\"\n",
    "first_row[1] = f\"box_threshold: {first_row[1]}\"\n",
    "first_row[2] = f\"text_threshold: {first_row[2]}\"\n",
    "first_row[3] = f\"box_size_threshold: {first_row[3]}\"\n",
    "\n",
    "# join the string by newline\n",
    "first_row = \"\\n\".join(first_row)\n",
    "\n",
    "# add the explanations to the first row\n",
    "top_5_df.loc[0,\"hyperparams_str\"] = first_row\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(top_5_df['hyperparams_str'], top_5_df['iou'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Hyperparameter combinations')\n",
    "plt.ylabel('IoU')\n",
    "plt.title('Top 5 Hyperparameters combinations by IoU')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df[[ \"prompt\", \"box_threshold\", \"text_threshold\", \"box_size_threshold\", \"iou\", \"f1_score\", \"precision\", \"recall\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "metrics_df['f1_score_cat'] = pd.qcut(metrics_df['f1_score'], q=10, labels=False)\n",
    "\n",
    "# Create the parallel coordinates plot\n",
    "fig = px.parallel_coordinates(metrics_df, \n",
    "                              dimensions=['f1_score', 'iou', 'accuracy', 'precision', 'recall'],\n",
    "                              color='f1_score_cat',\n",
    "                              labels={'f1_score': 'f1_score',\n",
    "                                      'iou': 'iou',\n",
    "                                      'accuracy': 'accuracy',\n",
    "                                      'precision': 'precision',\n",
    "                                      'recall': 'recall'},\n",
    "                              color_continuous_scale=px.colors.diverging.Tealrose,\n",
    "                              color_continuous_midpoint=2)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
